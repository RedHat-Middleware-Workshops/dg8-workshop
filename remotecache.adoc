== Using Cache on a Remote server


=== HotRod Client
Hot Rod is a binary TCP protocol that Infinispan offers high-performance client-server interactions with the following capabilities:
- Load balancing. Hot Rod clients can send requests across Infinispan clusters using different strategies to increase reliability.
- Failover. Hot Rod clients can monitor Infinispan cluster topology changes and automatically switch to available nodes to ensure requests never go to offline nodes.
- Efficient data location. Hot Rod clients can find key owners and make requests directly to those nodes, which reduces latency.

=== RemoteCache
The collection methods keySet, entrySet and values are backed by the remote cache. That is that every method is called back into the RemoteCache. This is useful as it allows for the various keys, entries or values to be retrieved lazily, and not requiring them all be stored in the client memory at once if the user does not want.

These collections adhere to the Map specification being that add and addAll are not supported but all other methods are supported.

One thing to note is the Iterator.remove and Set.remove or Collection.remove methods require more than 1 round trip to the server to operate. You can check out the RemoteCache Javadoc to see more details about these and the other methods.


=== Project details
For this example we are going to create a simple web application. it will take some input from a webform and then add the entries into the Cache. 
However in this case we will be using the RemoteCacheManager and we will be using ProtoStream API. All of this with a Quarkus based application. 

So let's get cracking. But first lets checkout the template project.

<TODO>


=== The Maven dependencies
Open the pom.xml file in the project.

We will be using the following dependencies to create our service

[source, maven, role="copypaste"]
----
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-resteasy</artifactId> <1>
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-resteasy-jsonb</artifactId> <2> 
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-infinispan-client</artifactId>
    </dependency>
    <dependency>
----

<1> quarkus-resteasy; for our REST endpoint
<2> quarkus-resteasy-jsonb; we will use this for Json serialization for our REST endpoint
<3> quarkus-infinispan-client; This extension will enable us to use RemoteCache


=== Protobuf
Protobuf or Protocol Buffers are a method of serializing structured data. Protocol buffers are a flexible, efficient, automated mechanism for serializing structured data; they are much smaller and simpler in expression. You can easily write and read your structured data to and from a variety of data streams and using a variety of languages. Protobuf is all about structured data, so the first thing to do is to define the structure of your data. This is accomplished by declaring Protobuf message types in .proto files. e.g the game.proto file looks like follows.

[source, protobuf, role="copypaste"]
----
package quickstart; <1> 

message Game{ <2>
    required string name = 2; <3>
    required string description = 3; <4>
}
----

Save the above content in the following file src/main/resources/META-INF/game.proto

<1> We define a package for our entity. 
<2> this is the name of our message. A message is similar to an entity. 
<3> we specify that our type `string name` is required.
<4> and same for our next field description. 


=== Marshallers
As described in the previous section, a fundamental concept of the Protobuf format is the definition of messages in the .proto schema to determine how an entity is represented. However, in order for our Java applications to utilise the Protobuf format to transmit/store data, it’s necessary for our Java objects to be encoded. This is handled by the ProtoStream library and its configured Marshaller implementations, which convert plain old Java objects into the Protobuf format.

Although generating resources is the easiest and most performant way to utilise ProtoStream, this method might not always be viable. For example, if you are not able to modify the Java object classes to add the required annotations. For such use cases, it’s possible to manually define the .proto schema and create a manual marshaller implementation. Lets define our Marshaller; Open the GameMarshaller class


Add the following method to our GameMarshaller. 
In the following code we specify how we are going to read from our ProtoStream. We could add any additional processing on the stream if we wanted to. For now we take a simplified read and return a Game Object. Hence everytime a stream is read from the Cache, this method will be called.

[source, java, role="copypaste"]
----
    @Override
    public Game readFrom(MessageMarshaller.ProtoStreamReader reader) throws IOException {
        String name = reader.readString("name");
        String description = reader.readString("description");
        return new Game(name, description);
    }
----

Next we can also define a writer method. It takes a Game object and translates that into a stream.

[source, java, role="copypaste"]
----
    @Override
    public void writeTo(MessageMarshaller.ProtoStreamWriter writer, Game game) throws IOException {
        writer.writeString("name", game.getName());
        writer.writeString("description", game.getDescription());
    }
----

Lets specify which class handles our Stream data. 

[source, java, role="copypaste"]
----
    @Override
    public Class<? extends Game> getJavaClass() {
        return Game.class;
    }
----

And finally here we let the Serialization process know what type we are doing this for. i.e. packagename.Class

[source, java, role="copypaste"]
----
    @Override
    public String getTypeName() {
        return "quickstart.Game";
    }
----

Perfect we have our Marshaller configured.

=== Configuring our RemoteCache
Let's move on and create our RemoteCache configuration

For this open the Init.java and add the following member variables to it. 

[source, java, role="copypaste"]
----
    public static final String GAME_CACHE = "games"; <1>

    @Inject
    RemoteCacheManager cacheManager; <2> 

    private static final String CACHE_CONFIG = <3>
            "<infinispan><cache-container>" +
                    "<distributed-cache name=\"%s\"></distributed-cache>" +
                    "</cache-container></infinispan>";
----

<1> First we specify a class level variable which is the name of our Cache. 
<2> We inject the cacheManager to our file. We only want to load the CacheManager once, and since its a heavy object, we want to do it at startup.
<3> As we learnt in the previous section we can also configure a cache with xml, we are exactly doing that here. We could have also loaded this from a file META-INF but for a short demo this works okay.

[source, java, role="copypaste"]
----
    void onStart(@Observes @Priority(value = 1) StartupEvent ev) {
        String xml = String.format(CACHE_CONFIG, "games"); <1>
        cacheManager.administration().getOrCreateCache(GAME_CACHE, new XMLStringConfiguration(xml)); <2>
    }
----

You might remember the onStart from our previous lab. We are doing the same thing here. 
<1> we use the xml defined in a String and pass it on to the Red Hat Data Grid server to parse it and create a new cache called games
<2> then we ask the cacheManager to get the Cache for us or create a new one if it doesnt exist. 

By now we should have a RemoteCacheManager configured, all we need to do now is to inject it in out REST resource.


=== REST endpoint

Open up the GameResource.java, this is our REST resource file use the resteasy dependencies. 

In the following code we inject or RemoteCache, and we specify which Remote cache we want by passing the variable GAME_CACHE to it, which we have initialized previously in our Init.java
Add this code to the GameResource.java

[source, java, role="copypaste"]
----
    @Inject
    @Remote(GAME_CACHE)
    RemoteCache<String, Game> gameStore;
----


The following are two simple GET and POST method implementation. 

[source, java, role="copypaste"]
----
    @GET
    public Set<Game> list() {
        return new HashSet<>(gameStore.values());
    }

    @POST
    public Set<String> add(Game game) {
        gameStore.putAsync(game.getName(), game);
        return gameStore.keySet();
    }
----

<1> the list method is simply posting back a HashSet back to the front-end
<2> and here the add method is using the Async api of infinispan/Red Hat Data Grid to add the entry into the cache.

Perfect. We are all set to deploy our application to Openshift and see how the RemoteCache will work.


=== Deployment: What's an Operator and how does it help us?
An Operator is a method of packaging, deploying and managing a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and kubectl tooling. An Operator is essentially a custom controller.
A controller is a core concept in Kubernetes and is implemented as a software loop that runs continuously on the Kubernetes master nodes comparing, and if necessary, reconciling the expressed desired state and the current state of an object. Objects are well known resources like Pods, Services, ConfigMaps, or PersistentVolumes. Operators apply this model at the level of entire applications and are, in effect, application-specific controllers.

The Operator is a piece of software running in a Pod on the cluster, interacting with the Kubernetes API server. It introduces new object types through Custom Resource Definitions, an extension mechanism in Kubernetes. These custom objects are the primary interface for a user; consistent with the resource-based interaction model on the Kubernetes cluster.

An Operator watches for these custom resource types and is notified about their presence or modification. When the Operator receives this notification it will start running a loop to ensure that all the required connections for the application service represented by these objects are actually available and configured in the way the user expressed in the object’s specification.

The Operator Lifecycle Manager (OLM) is the backplane that facilitates management of operators on a Kubernetes cluster. Operators that provide popular applications as a service are going to be long-lived workloads with, potentially, lots of permissions on the cluster.

With OLM, administrators can control which Operators are available in what namespaces and who can interact with running Operators. The permissions of an Operator are accurately configured automatically to follow a least-privilege approach. OLM manages the overall lifecycle of Operators and their resources, by doing things like resolving dependencies on other Operators, triggering updates to both an Operator and the application it manages, or granting a team access to an Operator for their slice of the cluster.

Red Hat Data Grid 8.0 comes with an Operator. The administrators of the cluster have already installed the Data Grid Operator, what we need to do as a user is define a Custom Resource as to how and what configuration we want for our Red Hat Data Grid instances. 

=== Installing
Assuming you have already logged in to openshift from the CodeReady terminal, if not you can do it now. Click on the `Login to Openshift` menu in the right menu called 'My Workspace'. Following is the screen shot showing you how to do that.

<TODO>


Lets start by installing a basic Red Hat Data Grid Cluster. 

[source, yaml, role="copypaste"]
----
    apiVersion: infinispan.org/v1
    kind: Infinispan <1>
    metadata:
    name: datagrid-service <2>
    spec:
    replicas: 2 <3>
----

Create a file with name cr_minimal.yaml copy and paste the above defination and save it.

Before applying this defination, lets take a look how its constructed. 
<1> tells Kubernetes/Openshift that the Custom resource type is Infinispan
<2> we specify the name of our cluster as datagrid-service
<3> and finally we specify the replicas we want for our service.

Now from the terminal use the oc command line to apply it. 
[source, shell, role="copypaste"]
----
oc apply -f cr_minimal.yaml
----

you can watch the Red Hat Data Grid Operator creating the instances by running the following command.

[source, shell, role="copypaste"]
----
oc get pods -w
----

Perfect now we have a running Red Hat Data Grid cluster. 


=== Deploying to Openshift and scaling

Lets prepare to deploy the application to Openshift

For this open up the application.properties located at src/main/resources/application.properties

[source, properties, role="copypaste"]
----

quarkus.infinispan-client.server-list=datagrid-service:11222<1>
quarkus.infinispan-client.client-intelligence=BASIC<2>
quarkus.infinispan-client.auth-username=developer<3>
quarkus.infinispan-client.auth-password=<4>


quarkus.http.cors=true

# Openshift extension settings.
quarkus.openshift.expose=true <5>

# if you dont set this and dont have a valid cert the deployment wont happen

quarkus.kubernetes-client.trust-certs=true<6>
----

<1> Sets the host name/port to connect to. Each one is separated by a semicolon (eg. host1:11222;host2:11222)
<2> Sets client intelligence used by authentication , in our case its basic, since we deployed a minimal server config
<3> Sets user name used by authentication, in our case its developer, thats the default from the operator.
<4> Sets password used by authentication, we do not have this yet. we will find it out from the secrets. 
<5> we make sure that our applications route will be exposed once its deployed.
<6> Finally we also put this property to true, incase our server does not have trusted certificates, which in our case can be true, since we are in a demo denvironment.

Lets go fill that password field in the above properties file.

Run the following command on the terminal and the password will be shown, then copy that password and add it to the password field `quarkus.infinispan-client.auth-password=`. 
[source, shell, role="copypaste"]
----
`oc get secret datagrid-service-generated-secret \                                                                                                                                                             
-o jsonpath="{.data.identities\.yaml}" | base64 --decode`
----

Save the application.properties file.


Lets go ahead and deploy the application to openshift. 
Open a terminal in CodeReady workspace and run the following command to build an s2i namespace for our applications image.

[source, shell, role="copypaste"]
----
mvn clean package -Dquarkus.container-image.build=true
----

You should see a Build Successful message from the above command. Once you have that message proceed to the second command below to deploy the application itself. 

[source, shell, role="copypaste"]
----
mvn clean package -Dquarkus.kubernetes.deploy=true
----

You should see a Build Successful message from this run as well. 

Now navigate to the openshift console

<TODO>


=== Enabling Near Cache
Near caches are optional caches for Hot Rod Java client implementations that keep recently accessed data close to the user, providing faster access to data that is accessed frequently. This cache acts as a local Hot Rod client cache that is updated whenever a remote entry is retrieved via get or getVersioned operations.

In Red Hat JBoss Data Grid, near cache consistency is achieved by using remote events, which send notifications to clients when entries are modified or removed (refer to Remote Event Listeners (Hot Rod)). With Near Caching, local cache remains consistent with remote cache. Local entry is updated or invalidated whenever remote entry on the server is updated or removed. At the client level, near caching is configurable as either of the following:

- *DISABLED* - the default mode, indicating that Near Caching is not enabled.
- *INVALIDATED* - enables near caching, keeping it in sync with the remote cache via invalidation messages.



image::nearcache.png[Near Caching, 900]


When should I use it? 
Near caching can improve the performance of an application when most of the accesses to a given cache are read-only and the accessed dataset is relatively small. When an application is doing lots of writes to a cache, invalidations, evictions and updates to the near cache need to happen. In such a scenario near cache wont be beneficial.

For Quarkus, near caching is disabled by default, but you can enable it by setting the profile config property quarkus.infinispan-client.near-cache-max-entries to a value greater than 0. You can also configure a regular expression so that only a subset of caches have near caching applied through the quarkus.infinispan-client.near-cache-name-pattern attribute.


Add the following properties to application.properties to enable near caching.

[source, shell, role="copypaste"]
----
infinispan.client.hotrod.near_cache.mode=INVALIDATED

infinispan.client.hotrod.near_cache.max_entries=40

infinispan.client.hotrod.near_cache.cache_name_pattern=*i8n-.
----

Lets go ahead and deploy the application to openshift. 
Open a terminal in CodeReady workspace and run the following command to build an s2i namespace for our applications image.

[source, shell, role="copypaste"]
----
mvn clean package -Dquarkus.container-image.build=true
----

You should see a Build Successful message from the above command. Once you have that message proceed to the second command below to deploy the application itself. 

[source, shell, role="copypaste"]
----
mvn clean package -Dquarkus.kubernetes.deploy=true
----

You should see a Build Successful message from this run as well. 



=== Caching with Hibernate and JPA