= What is Caching?

Modern applications use data and lets assume lots of data, whether it comes from databases, files, webservices, rpc call etc. 
When an application has to process this data, what is the most natural way of doing it? Mostly the application will process this data in memory. 
Lets assume I have a very slow database, this could be due to any reason, network latency, or even big queries that return alot of data. 
So the most straight forward way of handling this, would be to store some of that data in the memory. By doing so, you would be able to process requests to your systems mush faster. However there are challenges. How much data should you store in memory? and most importantly what will happen in case of failure scenarios? 

- Will you loose all the state of your data in memory? 
- Will you need to re-read all your data and events inorder to get back to the same state where you failed. Or you might have to let go of that entirely. 

Now the above two might seem very simple, but that tasks can be tideous and most importantly error prone. 
So at this point we could introduce a local cache that e.g. could be a ConcurrentHashMap. and alot of us might have done this in the past. However as a developer you might know that this has not much effect in case of failures and the handling that you will need to do incase of failures. 
So the need is entirely for a component that can not just cache data in the memory, but give 

1. A consistent way to handle data and state in the memory. 
2. Resiliencey in case of failures. 
3. Processing efficency and performance.
4. Events, streams, and distribution capabilites. 


image::caching.png[Caching, 900]


By having such capabilites a cache is no longer just a datastructure in the memory, but also as a developer now you have the possiblity to take this component out of your local in memory processing and distribute it out on the network. Thereby incase of application failures you will still be able to access this data from the last point where you left off. 

Now getting back to our primary question, how much data should you store in memory? Partially we have already discussed this above. Whats important is that as a developer you should be able to specifiy TTL (Time To Live) for your cache and its entries. You should be able to define eviction and expiration. There by knowing when your cash is hot and what data resides in it. Most over you should be able to do this distributed, cluster wide or remotely. 

Once a cache is remote, we also want some of the distributed features, e.g. monitoring. 
Lets take a look at some of the caching strategies.

==== Local cache
The primary use for Red Hat Data Grid is to provide a fast in-memory cache of frequently accessed data. Suppose you have a slow data source (database, web service, text file, etc): you could load some or all of that data in memory so that itâ€™s just a memory access away from your code. Using Red Hat Data Grid is better than using a simple ConcurrentHashMap. By setting up an embedded cache, Red Hat Dat Grid also allows you to tap into more features e.g. expiration, eviction, events on the cache etc. All make out a much better way of handling your cache and component design. Moreover if you would want to cluster such a cache that is also easily possible. 

==== As a clustered cache
So lets assume you started with a local embedded cache in your application, and now you suddenly realize that one instance of your application is not enough to handle the load from your users or systems. What do you do? With Red Hat Data Grid you can now scale that cache into a cluster. You dont need to change how you use your cache, but adding a few additonal config params you can now have a clustered cache and there by having muliptle instances of your application listenting to the same coherent cache. Events will be fired accorss the clusters, your eviction and expiration will happen accorss the cluster. 
And most over, you now even have the possiblity to distribute your keys accross the cluster. Red Hat Data Grid can scale horizontally to hundreds of nodes. 

==== As a remote cache
Lets just say you used the clusterd cache, and embedded it in your application, which means that everytime a new instance of your application started you would have a new instance of your embedded cache ready to become part of the cluster. Now this is all great. But what if, you dont want that clustering in your application. rather then you might want to use a component outside of your applications lifecycle. Or you would want to share this cache accross multiple applications. In that case the Red Hat Data Grid could be used as a remote data grid. Now you can access your cache via multiple programming runtimes. e.g. Vert.x, Quarkus, NodeJS, C#, C/C++ etc. And your cache lifecycle will be independant of the applications life cycle, which is a great advantage in many cases. 


Congratulations! By now you understand the different patterns of caching, and the requirements. Lets go ahead and create our first application and learn how we can use Red Hat Data Grid to achieve caching. Press next! 



