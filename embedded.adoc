What could be cases where you want to use Cache; Lets take a moment and think about it. Where do you think you could use cache? 
Well there could be limitless answers to this question; Some common usecases are listed below

- Lookup Data; if you have a app, that needs some user profile data etc, thats unecessary to pole everytime, and it doesnt change much
- Latency or bulk; you might have a service or database that takes alot of time to load some of the data.
- Traffic; you might have loads of users and trends are spiking
- Session Storage; Storing your webapp sessions, this could be carts etc, that you can use to scale your application
- Global Counters; You might want to create distributed keys accross a distributed dataset. Use this to update and fetch data. 

image::embeddedcache.png[Caching, 900]

The above figure depicts a common scenario when you dont want to use a networked dataset that could be slow and with high latency, but rather use an inmemory cache. In this case the in-memory cache becomes the front for the data. 

Our first lab is about an embedded or local cache, also a common usecase for applications.  What is an Embedded Cache in Red Hat Data Grid?

=== Embedded Cache, Some basics
The CacheManager is Red Hat Data Grid’s main entry point. You can use a CacheManager to

- configure and obtain caches
- manage and monitor your nodes
- execute code across a cluster


Depending on whether you are embedding Red Hat Data Grid in your application or you are using it remotely, you will be dealing with either an EmbeddedCacheManager or a RemoteCacheManager. While they share some methods and properties, be aware that there are semantic differences between them.

CacheManagers are heavyweight objects, and recommended use would be one CacheManager used per JVM (unless specific setups require more than one; but either way, this should be a minimal and finite number of instances).

The simplest way to create a CacheManager is:

[source, java, role="copypaste"]
----
EmbeddedCacheManager manager = new DefaultCacheManager();
----

which starts the most basic, local mode, non-clustered cache manager with no caches. CacheManagers have a lifecycle and the default constructors also call start(). Overloaded versions of the constructors are available, that do not start the CacheManager, although keep in mind that CacheManagers need to be started before they can be used to create Cache instances.

Once constructed, CacheManagers should be made available to any component that require to interact with it via some form of application-wide scope such as JNDI, a ServletContext or via some other mechanism such as an IoC container.

When you are done with a CacheManager, you must stop it so that it can release its resources:

[source, java, role="copypaste"]
----
manager.stop();
----

This will ensure all caches within its scope are properly stopped, thread pools are shutdown. If the CacheManager was clustered it will also leave the cluster gracefully. 

== Your first service with Caching
One of the common usecases for a Cache is to keep most used data in memory. Example having a Scoreboard in the cache. Lets assume theres a webpage that keeps the Score card for a round played by players on different tours. Now since this website expects people coming to check the top scores for example, or maybe based on a country etc. The best approach would be store this information in a cache rather then polling that information from different webservices or different databases as an example. 

In our first service we will do exactly that. We will store this data in our Embedded Cache and understand not only how this works but the different ways of handling cache, getting events from it etc. 

=== Project details
You can choose multiple runtimes to impelement this service, in our case today the example uses Quarkus. A Kubernetes Native Java stack tailored for OpenJDK HotSpot and GraalVM, crafted from the best of breed Java libraries and standards. Quarkus is also known for some of its cool features e.g Live Coding which we will also use in our labs. It makes it easier to code and see our changes as we do that. 

Time to checkout our template project. 

You will be using Red Hat CodeReady Workspaces, an online IDE based on Eclipe Che{:target="_blank"}{:target="_blank"}. Changes to files are auto-saved every You will be using Red Hat CodeReady Workspaces, an online IDE based on https://www.eclipse.org/che/[Eclipe Che, window=_blank]. *Changes to files are auto-saved every few seconds*, so you don’t need to explicitly save changes.

To get started, {{ ECLIPSE_CHE_URL }}[access the Che instance, window=_blank] and log in using the username and password you’ve been assigned (e.g. `{{ USER_ID }}/{{ CHE_USER_PASSWORD }}`):

image::che-login.png[cdw, 700, align="center"]

Once you log in, you’ll be placed on your personal dashboard. Click on the name of the pre-created workspace on the left, as shown below (the name will be different depending on your assigned number). You can also click on the name of the workspace in the center, and then click on the green button that says _Open_ on the top right hand side of the screen.

This IDE is based on Eclipse Che (which is in turn based on MicroSoft VS Code editor).

You can see icons on the left for navigating between project explorer, search, version control (e.g. Git), debugging, and other plugins.  You’ll use these during the course of this workshop. Feel free to click on them and see what they do:

image::crw-icons.png[cdw, 400, align="center"]

[NOTE]
====
If things get weird or your browser appears, you can simply reload the browser tab to refresh the view.
====

Many features of CodeReady Workspaces are accessed via *Commands*. You can see a few of the commands listed with links on the home page (e.g. _New File.._, _Git Clone.._, and others).

If you ever need to run commands that you don't see in a menu, you can press kbd:[F1] to open the command window, or the more traditional kbd:[Control+SHIFT+P] (or kbd:[Command+SHIFT+P] on Mac OS X).

Let's import our first project. Click on **Git Clone..** (or type kbd:[F1], enter 'git' and click on the auto-completed _Git Clone.._ )

image::gitclonepage.png[cdw, 600, align="center"]

Step through the prompts, using the following value for **Repository URL**:

[source, shell, role="copypaste"]
----

https://github.com/sshaaf/dg8-embedded-quarkus.git

----

image::gitcloneembedded.png[crw, 600, align="center"]

Next, select `$CHE_PROJECTS_ROOT` in the drop-down menu for destination directory:

image::projectplace.png[crw, 600, align="center"]

And click *Select Repository Location*.

Once imported, choose **Add to workspace** when prompted.

The project should now be imported into your workspace and as an example screenshot as below you should be able to see your project as well.`dg8-embedded-quarkus`

image::workspaceview.png[crw, 600, align="center"]

[NOTE]
====
The Terminal window in CodeReady Workspaces. You can open a terminal window for any of the containers running in your Developer workspace. For the rest of these labs, anytime you need to run a command in a terminal, you can use the **>_ New Terminal** command on the right:
====

image::codeready-workspace-terminal.png[codeready-workspace-terminal, 600, align="center"]

As you can see there is already some files inplace. Lets take a look into what these files are and do.


=== The Maven dependencies
Open the pom.xml file in the project.

We will be using the following dependencies to create our service

[source, maven, role="copypaste"]
----
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-resteasy</artifactId> <1>
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-resteasy-jsonb</artifactId> <2> 
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-infinispan-embedded</artifactId> <3> 
    </dependency>
    <dependency>
----

<1> Quarkus-resteasy; for our REST endpoint
<2> Quarkus-resteasy-jsonb; we will use this for Json serialization for our REST endpoint
<3> Quarkus-infinispan-embedded; This extension will enable us to embed our cache in our service.


=== The Score Entity
We have also created a POJO called Score, which will serve as our datastructure for the ScoreCard. If you have played golf, you might wonder this is a very basic data structure and that's entirely true, we could have gone in more details but we have kept this short to cover all the features. And you are welcome to extending this datastructure after successfully finishing these labs. 

If you open `Score.java` you will see the following first few lines

[source, java, role="copypaste"]
----
    // The number of holes played per round
    public static final int HOLES = 18;

    // The players is on this hole
    private int currentHole = 0;

    // Name of the player
    private String playerName;

    // players unique Id
    private String playerId;

    // The actual scoreCard
    private int[] card = new int[HOLES];

    // The course player is playing on.
    private String course = "St.Andrews Links";

    // the courseCard; the expected handicap
    private int[] courseCard = {4,4,4,4,5,4,4,3,4,4,3,4,4,5,4,4,4,4};

----

The rest of the methods are accessors for these fields. Important to mention we do have three constructors

[source, java, role="copypaste"]
----
 
    // Used in Json serialization
    public Score()

    // Creating a new player with course and the courses score card
    public Score(String playerName, String playerId, String course, int[] courseCard)

    // Creating a new player with defaults
    public Score(String playerName, String playerId)
----

Take a look at some of the other methods in the Score class and make yourself familiar with it. Do not change the class at this time. 


=== Creating a service for caching
So now that you are familiar with the project template, lets start by creating a service. Todo this open ScoreService.java

Define the following three class level variables

[source, java, role="copypaste"]
----
 
    Cache<Object, Score> scoreCache; <1> 

    Logger log = LoggerFactory.getLogger(ScoreService.class); <2> 

    @Inject
    EmbeddedCacheManager cacheManager; <3> 

----

<1> the scoreCache is an instance of Cache, which will be our point to store and retrieve values. Cache expects <K,V> types, in our case our key is an Object and our actual entry is a Score. Yes the same Score POJO we say earlier. The Cache is also the central interface of Red Hat Data Grid. A Cache provides a highly concurrent, optionally distributed data structure with additional features such as; JTA transaction compatibility, Eviction support for evicting entries from memory to prevent OutOfMemoryErrors, Persisting entries to a CacheLoader, either when they are evicted as an overflow, or all the time, to maintain persistent copies that would withstand server failure or restarts. For convenience, Cache extends ConcurrentMap and implements all methods accordingly. Methods like keySet(), values() and entrySet() produce backing collections in that updates done to them also update the original Cache instance. Certain methods on these maps can be expensive however (prohibitively so when using a distributed cache). The size() and Map.containsValue(Object) methods upon invocation can also be expensive just as well. The reason these methods are expensive are that they take into account entries stored in a configured CacheLoader and remote entries when using a distributed cache.
<2> the log; straight forward logger incase we want to log something. 
<3> cacheManager; which is an instance of EmbeddedCacheManager, we inject this into our code using the dependency injection and this is possible due to the extension we added in our maven dependencies. 


Next let's create some accessor methods for our service. 

[source, java, role="copypaste"]
----
    public List<Score> getAll() { <1>
        return new ArrayList<>(scoreCache.values());
    }

    public void save(Score entry) { <2> 
        scoreCache.put(getKey(entry), entry);
    }

    public void delete(Score entry) { <3> 
        scoreCache.remove(getKey(entry));
    }

    public void getEntry(Score entry){ <4> 
        scoreCache.get(getKey(entry));
    }

----

<1> We get all values from the cache and return them as a List of Scores
<2> We are saving the entire entry, which we expect as a Score object.
<3> We are deleting an entry from our cache
<4> Finally we want to get 1 entry from our cache. 

These are simple accessor methods, one thing you might have noticed is the use of the method `getKey`. This method described as follows has one simple task i.e. to make get us the key, which in our case we use as a concatanted string of playerId+course. Since entry always has both of these values we concatenate them here. 

Add the following method to your class as well.

[source, java, role="copypaste"]
----
public static String getKey(Score entry){
        return entry.getPlayerId()+","+entry.getCourse();
    }
----


Perfect! Almost to our final step for this service. What we are missing is initialization of our CacheManager and then we need to ask the CacheManager to give us a new cache. 

The CacheManager has many purposes:
- acts as a container for caches and controls their lifecycle
- manages global configuration and common data structures and resources (e.g. thread pools)
- manages clustering

A CacheManager is a fairly heavy-weight component, and you will probably want to initialize it early on in your application lifecycle.
For that reason we use the onStart method in this Service to ensure that the CacheManager and Cache are both created at startup. This also benefits us when we change this to clustering mode, more on that in our next lab. 


[source, java, role="copypaste"]
----
    void onStart(@Observes @Priority(value = 1) StartupEvent ev){
        cacheManager = new DefaultCacheManager(); <1>
        ConfigurationBuilder config = new ConfigurationBuilder(); <2>

        cacheManager.defineConfiguration("scoreboard", config.build()); <3> 
        scoreCache = cacheManager.getCache("scoreboard"); <4> 

        log.info("Cache initialized");

    }
----

<1> Constructing a CacheManager is done via one of its constructors, which optionally take in a Configuration or a path or URL to a configuration XML file. In our current config we do not need to add much, but use the defaults
<2> We use defaults for the Configuration builder. its a very handy Object that enables us to define different cache configurations which we will notice further on in this lab. 
<3> We are passing our configuration to the CacheManager.
<4> You obtain Cache instances from the CacheManager by using one of the overloaded getCache(), methods. Note that with getCache(), there is no guarantee that the instance you get is brand-new and empty, since caches are named and shared. Because of this, the CacheManager also acts as a repository of Caches, and is an effective mechanism of looking up or creating Caches on demand. In our case we expect this to be the first Cache and local embedded one. This is also not clustered. 


[NOTE]
====
You might have noticed, that a CacheManager can have multiple Caches; which is great, since in any application you could store multiple unrelated data in different caches, not just that you might even want to have different behaviour with different Caches, e.g. Eviction or Expiration could differ etc. This gives us a lot more to work with then we would in a ConcurrentHashMap as an example.
====


=== Creating a REST Resource for our app
Lets create our REST resource. This should be simple. Open the ScoreResource.java file. 
Since we already implemented most of our code in the service, we need to make sure we can respond on the correct REST calls. 

First lets inject our ScoreService so we can use all the caching functions we need.
[source, java, role="copypaste"]
----
    @Inject
    ScoreService scoreService;
----


Lets implement the create end point, here we are simply calling the save function on the scoreService.
[source, java, role="copypaste"]
----
    @POST
    @Transactional
    public Response create(@Valid Score item) {
        scoreService.save(item);
        return Response.status(Status.CREATED).entity(item).build();
    }
----

And we also want to be able to get one entry from our cache. following method will do that by calling the scoreService.findById
[source, java, role="copypaste"]
----
    @GET
    @Path("/{id}")
    public Object getOne(@PathParam("id") String id) {
        Object entity = scoreService.findById(id);
        if (entity == null) {
            throw new WebApplicationException("ScoreCard with id of " + id + " does not exist.", Status.NOT_FOUND);
        }
        return entity;
    }
----

And incase we wanted to update an entry. that would normally the case when we the player is playing the round. so the score will be updated. 
[source, java, role="copypaste"]
----
    @PATCH
    @Path("/{id}")
    @Transactional
    public Response update(@Valid Score card, @PathParam("id") Long id) {
        scoreService.save(card);
        return Response.status(Status.CREATED).entity(card).build();

    }
----

Take a look into some of the other methods in the ScoreResource to make your self familiar with the code there.

If you might have noticed at the class declaration we are using the following annotations

[source, java, role="copypaste"]
----
@Produces(MediaType.APPLICATION_JSON) <1>
@Consumes(MediaType.APPLICATION_JSON) <2>
@Path("/api") <3>
----

<1> This means we are producing JSON from our responses
<2> This means we only listen to JSON, this helps us to consume the JSON directly and serialize it into our Score POJO as an example.
<3> and `api` is the path to our resource. e.g. localhost:8080/api

=== Run the Service
<TODO>


=== Expiration of Entries
Lets assume you are pulling this data off from a database. You might want that it should be removed from the cache after a certain time period. 
You can do this by defining this either on the a single entry or the entire cache. By default entries created are immortal and do not have a lifespan or maximum idle time. Using the cache API, mortal entries can be created with lifespans and/or maximum idle times

Expiration is a top-level construct, represented in the configuration as well as in the cache API.
- While eviction is local to each cache instance , expiration is cluster-wide . Expiration lifespan and maxIdle values are replicated along with the cache entry.
- Maximum idle times for cache entries require additional network messages in clustered environments. For this reason, setting maxIdle in clustered caches can result in slower operation times.
- Expiration lifespan and maxIdle are also persisted in CacheStores, so this information survives eviction/passivation.

Lets start with doing this for one entry. 

In Infinispan entry expiration can happen in two ways:

- a certain time after the data was inserted into the cache (i.e. lifespan)
- a certain time after the data was last accessed (i.e. maximum idle time)

The Cache interface offers overloaded versions of the put() method that allow specifying either or both expiration properties. The following example shows how to insert an entry which will expire after 5 seconds

Open the ScoreService and change the save method to the following.

[source, java, role="copypaste"]
----
    public void save(Score entry) {  
        scoreCache.put(getKey(entry), entry, 5, TimeUnit.SECONDS););
    }
----

In the above code, we have used TimeUnit and we specify 5 as the unit which is seconds. Following are the units you can use in the TimeUnit
[source, java, role="copypaste"]
----
    NANOSECONDS,
    MICROSECONDS,
    MILLISECONDS,
    SECONDS,
    MINUTES,
    HOURS,
    DAYS;
----

In the previous step we used the overloaded put() method to store mortal entries. But since we want all of our entries to expire with the same lifespan, we can configure the cache to have default expiration values. To do this we will construct the DefaultCacheManager by passing in a org.infinispan.configuration.cache.Configuration object. A configuration in Infinispan is mostly immutable, aside from some runtime-tunable parameters, and is constructed by means of a ConfigurationBuilder. Using the above use-case, let's create a cache configuration where we want to set the default expiration of entries to 5 seconds. 
Add the following line to your ScoreService onStart method; right under the `ConfigurationBuilder` instantiation 

[source, java, role="copypaste"]
----
    config.expiration().lifespan(5, TimeUnit.SECONDS);
----

Now this is a configuration change for the cache and this will expire all entries after 5 seconds. 

Next task for you is to change the lifespan to 5 minutes. 

[NOTE]
====
When an entry expires it resides in the data container or cache store until it is accessed again by a user request. An expiration reaper is also available to check for expired entries and remove them at a configurable interval of milliseconds. More information can be found in the Product documentation
====


=== Eviction

Red Hat Data Grid supports eviction of entries, such that you do not run out of memory. Eviction is typically used in conjunction with a cache store, so that entries are not permanently lost when evicted, since eviction only removes entries from memory and not from cache stores or the rest of the cluster. Red Hat Data Grid supports storing data in a few different formats. Data can be stored as the object iself, binary as a byte[], and off-heap which stores the byte[] in native memory.

[NOTE]
====
Eviction occurs on a local basis, and is not cluster-wide. Each node runs an eviction thread to analyse the contents of its in-memory container and decide what to evict. Eviction does not take into account the amount of free memory in the JVM as threshold to starts evicting entries. You have to set size attribute of the eviction element to be greater than zero in order for eviction to be turned on. If size is too large you can run out of memory. The size attribute will probably take some tuning in each use case.
====

Add the following line to your ScoreService onStart method; right under the `ConfigurationBuilder` instantiation 

[source, java, role="copypaste"]
----
        Configuration c = new ConfigurationBuilder()
                .memory()
                .storageType(StorageType.BINARY)
                .evictionType(EvictionType.MEMORY) <1>
                .size(1_000_000_000)
                .build();

----

<1> Eviction type applies only when the size is set to something greater than 0. The eviction type below determines when the container will decide to remove entries.

*COUNT*
This type of eviction will remove entries based on how many there are in the cache. Once the count of entries has grown larger than the size then an entry will be removed to make room.

*MEMORY*
This type of eviction will estimate how much each entry will take up in memory and will remove an entry when the total size of all entries is larger than the configured size. This type does not work with OBJECT storage type below.


==== Eviction Strategies
Following strategies can also be selected.  

*NONE*
Eviction is not enabled and it is assumed that the user will not invoke evict directly on the cache. If passivation is enabled this will cause aa warning message to be emitted. This is the default strategy.

*MANUAL*
This strategy is just like <b>NONE</b> except that it asssumes the user will be invoking evict directly. This way if passivation is enabled no warning message is logged.

*REMOVE*
This strategy will actually evict "old" entries to make room for incoming ones.

Eviction is handled by Caffeine utilizing the TinyLFU algorithm with an additional admission window. This was chosen as provides high hit rate while also requiring low memory overhead. This provides a better hit ratio than LRU while also requiring less memory than LIRS.

*EXCEPTION*
This strategy actually prevents new entries from being created by throwing a ContainerFullException. This strategy only works with transactional caches that always run with 2 phase commit, that is no 1 phase commit or synchronization optimizations allowed.


=== Difference between Eviction and Expiration

Both Eviction and Expiration are means of cleaning the cache of unused entries and thus guarding the heap against OutOfMemory exceptions, so now a brief explanation of the difference.

- With eviction you set maximal number of entries you want to keep in the cache and if this limit is exceeded, some candidates are found to be removed according to a choosen eviction strategy (LRU, LIRS, etc…​). Eviction can be setup to work with passivation, which is eviction to a cache store.

- With expiration you set time criteria for entries to specify how long you want to keep them in the cache.

- *lifespan* Specifies how long entries can remain in the cache before they expire. The default value is -1, which is unlimited time.

- *maximum idle time* Specifies how long entries can remain idle before they expire. An entry in the cache is idle when no operation is performed with the key. The default value is -1, which is unlimited time.


Perfect now we know what eviction and expiration API we have at our disposal and how we can use them in our app. 

=== Listeners
Red Hat Data Grid offers a listener API, where clients can register for and get notified when events take place. This annotation-driven API applies to 2 different levels: cache level events and cache manager level events.

Events trigger a notification which is dispatched to listeners. Listeners are simple POJO s annotated with @Listener and registered using the methods defined in the Listenable interface.

Both Cache and CacheManager implement Listenable, which means you can attach listeners to either a cache or a cache manager, to receive either cache-level or cache manager-level notifications.

Implement a new class `CacheListener`

[source, java, role="copypaste"]
----
import org.infinispan.notifications.Listener;
import org.infinispan.notifications.cachelistener.annotation.CacheEntryCreated;
import org.infinispan.notifications.cachelistener.annotation.CacheEntryModified;
import org.infinispan.notifications.cachelistener.event.CacheEntryCreatedEvent;
import org.infinispan.notifications.cachelistener.event.CacheEntryModifiedEvent;

@Listener(clustered = true)
public class CacheListener {

    @CacheEntryCreated
    public void entryCreated(CacheEntryCreatedEvent<String, Score> event) {
        System.out.printf("-- Entry for %s created \n", event.getType());
    }

    @CacheEntryModified
    public void entryUpdated(CacheEntryModifiedEvent<String, Score> event){
        System.out.printf("-- Entry for %s modified\n", event.getType());
    }
}
----

Also important is to add this listener to our Cache configuration. 
Add the following line to the config

[source, java, role="copypaste"]
----
    scoreCache.addListener(new CacheListener());
----

Now if we update the entries in our cache or create new ones; we will see a notification on the our console. 



=== Recap
<1> You created our own Cache and learnt how to us EmbeddedCacheManager
<2> You learnt how to use ConfigurationBuilder and Configuration objects to define our Configurations for the Cache and CacheManager
<3> You learnt about Expiration and Eviction
<4> And lastly you implemented your own Listener. 

**Congratulations!!* you have completed the first lab of this workshop. Lets move to the next lab and learn how we can cluster this Cache and also deploy this on a cloud environment like Openshift.

