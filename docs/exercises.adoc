= Getting started with Data Grid
:experimental:

Let's delve down into some of the basic features of a Red Hat Data Grid. Our workshop includes a lot more in-depth material going forward, but it's essential we set up a solid base with understanding more about Red Hat Data Grid and Infinispan. In this section, we have prepared six exercises for you. These exercises give a basic introduction to some of the features of the Red Hat Data Grid Java API.

=== About your workspace environment
For developments and deployment, we use the using Red Hat CodeReady Workspaces, an online IDE based on https://www.eclipse.org/che/[Eclipe Che, window=_blank]. *Changes to files are auto-saved every few seconds*, so you don't need to save changes explicitly.

To get started, {{ CHE_URL }}[access the CodeReady Workspaces instance^] and log in using the username and password you are assigned (e.g. `{{ USER_ID }}/{{ CHE_USER_PASSWORD }}`):

image::che-login.png[cdw, 700, align="center"]

By logging in to CodeReady, you get access to your development workspace. We have already created a workspace for you. Your development environment opens up by clicking the workspace on the left menu. 

You can see icons on the left for navigating between project explorer, search, version control (e.g., Git), debugging, and other plugins. You'll use these during this workshop. Feel free to click on them and see what they do:

image::crw-icons.png[cdw, 400, align="center"]

[NOTE]
====
If things get weird or your browser appears, you can simply reload the browser tab to refresh the view.
====

Many features of CodeReady Workspaces are accessed via *Commands*. You can see a few of the commands listed with links on the home page (e.g., _New File.._, _Git Clone.._, and others).

If you ever need to run commands that you don't see in a menu, you can press kbd:[F1] to open the command window, or the more traditional kbd:[Control+SHIFT+P] (or kbd:[Command+SHIFT+P] on Mac OS X).

Let's import our first project. Click on **Git Clone..** (or type kbd:[F1], enter 'git' and click on the auto-completed _Git Clone.._ )

image::gitclonepage.png[cdw, 600, align="center"]

Step through the prompts, using the following value for **Repository URL**. If you use *FireFox*, it may end up pasting extra spaces at the end, so just press backspace after pasting:

[source, shell, role="copypaste"]
----

https://github.com/RedHat-Middleware-Workshops/dg8-workshop-labs

----

image::gitcloneembedded.png[crw, 600, align="center"]

The project is now imported into your workspace. Following screenshot shows the workspace after the lab projects have been imported.

1. On the left you can see the project explorer with the heading *EXPLORER:PROJECTS*. Project explorer can be used to navigate to source files. Once you click any source file. it will open up in the editor.
2. On the right is the `Workspace Command View` with the heading *MYWORKSPACE:WORKSPACE*. In this view we have created point and click commands. These commands will be used through out the workshop labs.

image::workspaceview.png[crw, 800, align="center"]



=== Exercises

This first lab comprises of 6 Exercises. The exercises will give you a glimpse into some of the features of Red Hat Data Grid, and is a good starting point to learn how to use Red Hat Data Grid with your applications. 

==== Summary of the Exercises

1. *Exercise 1* - Creating a Cache
2. *Exercise 2* - JSR-107, JCache
3. *Exercise 3* - Functional API
4. *Exercise 4* - Streaming data from the Cache
5. *Exercise 5* - Using Transactions
6. *Exercise 6* - Queries to the Cache with Lucene

Each Exercise has a corresponding .java file e.g. `Exercise1.java`. The maven project required for this lab is `dg8-exercises`. Following screenshot shows where the Java files are placed. The package name we have used is `org.acme`

image::workspaceview-exercises.png[exercises, 800, align="center"]


All the exercises are marked with a `//TODO:`. Where ever you see this `//TODO:` it signifies that you need to write some code below it. We have added some comments with it, so you know what is required. Incase if the instructions are not understandable, please ask the instructor.

Moreover you will find that preceding exercise sections will explain the `//TODO` in more details and what needs to be done.


=== Exercise 1: Creating a local Cache
First, a bit about `Map`s. Why are `Map`s good for a cache? `Map`s are fast, they use methods like `hashCode` and `equals` to determine how to add data to the map. This also means they can be fast enough (O(1)) time to read and write the data. That is exceptional performance, and that's what one would expect from a cache. Data storage is in key and value pairs. There is a lot more to `Map`s, but let's start with a basic cache how-to.

A *CacheManager* is the primary mechanism for retrieving a Cache instance and is often used as a starting point to using the cache.
Essentially if you were using a `Map` object you would just create a `Map` and store all your key|value pairs in it. However, when you use a tool like Red Hat Data Grid/Inifinispan, you get more than just a simple map (e.g. Listeners, events, etc), all of which we will talk about in further sections. 

CacheManagers are heavyweight objects, and it's not recommended to have more than one *CacheManager* being used per JVM (unless specific configuration requirements require more than one, but either way, this would be a minimal and finite number of instances). 

Add the following to your main method in class Exercise1

[source, java, role="copypaste"]
----
    // TODO: Construct a simple local cache manager with default configuration
    DefaultCacheManager cacheManager = new DefaultCacheManager();
----

Now that we have `cacheManager`, we can now define what a Cache should look like. We could choose many features from the system (e.g. if we were adding grouping, streams, listeners, strategies for eviction or clustering, etc) we would do that here. The following example just takes the default configuration.

[source, java, role="copypaste"]
----
    // TODO: Define local cache configuration
    cacheManager.defineConfiguration("local", new ConfigurationBuilder().build());
----

Perfect, so now we have defined our cache, time for us to get that cache from our *CacheManager*. We have also defined that our cache should have both our Key and Value as `Strings`.

[source, java, role="copypaste"]
----
    // TODO: Obtain the local cache
    Cache<String, String> cache = cacheManager.getCache("local");
----

Finally lets put an entry in the Cache. Change the "key" and "value" to anything you'd like. 

[source, java, role="copypaste"]
----
    // TODO: Store a value
    cache.put("key", "value");
----

Here we get the value by specifying the key. The key is the same as we used in our previous line's `cache.put`. By specifying a key to the cache, you can get the value stored in it; the same process is also used for an update.

[source, java, role="copypaste"]
----
    // TODO: Retrieve the value and print it out
    System.out.printf("key = %s\n", cache.get("key"));
----

Finally, *CacheManager* is a heavy object; it does a lot, so no need to keep it going on. When done, we close that instance by calling the `stop()` method.

[source, java, role="copypaste"]
----
    // TODO: Stop the cache manager and release all resources
    cacheManager.stop();
----

Great, now we have all we require to run this Exercise. Let's try to run it.
You can choose to run it via the Workspace command view by clicking on `Exercise1`. Or you can just open a new terminal from the same view `>_ New Terminal` and run the it manually using maven commands. Both methods would work.

[IMPORTANT]
====

Remember incase of running maven directly via terminal the path to the exercises project is as follows. `/projects/dg8-workshop-labs/dg8-exercises`. Make sure you are in this directory before you run maven commands from the terminal.

====

[source, shell, role="copypaste"]
----
mvn clean compile && \
mvn exec:java -Dexec.mainClass=org.acme.Exercise1
----

You should be able to see an output similar to the following.
[source, shell, role="copypaste"]
----
Jun 22, 2020 6:40:02 PM org.infinispan.factories.GlobalComponentRegistry preStart
INFO: ISPN000128: Infinispan version: Red Hat Data Grid 'Turia' 10.1.5.Final-redhat-00001
Jun 22, 2020 6:40:03 PM org.infinispan.lock.impl.ClusteredLockModuleLifecycle cacheManagerStarted
INFO: ISPN029009: Configuration is not clustered, clustered locks are disabled
----

=== Exercise 2: JSR-107 JCache
The term cache is generally referred to as a component that stored data in memory so that its easy to read the value that might be hard to calculate or that need to be accessed rather quickly. As discussed earlier, simple `java.util` packages do now have all the capabilities required, and wiring them by oneself is complex if not hard enough. The Java Specification Request (JSR-107) has been created to define temporary caching API for Java. The specification defines some Standard APIs for storing and managing data both for local and distributed use cases.

Let's take a look at how you can use JSR-107 with Red Hat Data Grid/Infinispan

[source, java, role="copypaste"]
----
        // TODO: Construct a simple local cache manager with default configuration
        CachingProvider jcacheProvider = Caching.getCachingProvider(); <1>
        CacheManager cacheManager = jcacheProvider.getCacheManager(); <2>
        MutableConfiguration<String, String> configuration = new MutableConfiguration<>(); <3>
        configuration.setTypes(String.class, String.class); <4>
        
        // TODO: create a cache using the supplied configuration
        Cache<String, String> cache = cacheManager.createCache("myCache", configuration); <5>
----
Let's take a more in-depth look at the code above

<1> We use a `CachingProvider`, which is part of the standards API
<2> The Caching provider, in turn, gives us a `CacheManager`
<3> We create a configuration object for our cache (in this case a `MutableConfiguration`)
<4> Here we also set the type of keys & values in our Cache (If you remember this is different from our previous exercise since we are using the JSR-107 API now)
<5> and finally we get our cache

Finally lets put an entry in the Cache. Change the "key" and "value" to anything you'd like.
[source, java, role="copypaste"]
----
        // Store and retrieve value
        cache.put("key", "value");
        System.out.printf("key = %s\n", cache.get("key"));
----

And then lets close our `CacheManager`.
[source, java, role="copypaste"]
----
        // TODO: Stop the cache manager and release all resources
        cacheManager.close();
----

Run the above exercise as follows in the CodeReady terminal, or you can also choose to execute the command `Exercise2` in your MyWorkspace Menu on the right.
[source, shell, role="copypaste"]
----
mvn clean compile && \
mvn exec:java -Dexec.mainClass=org.acme.Exercise2
----

You should be able to see an output similar to the following. On the last line you can see your key, value printed.
[source, shell, role="copypaste"]
----
Jun 22, 2020 6:54:25 PM org.infinispan.factories.GlobalComponentRegistry preStart
INFO: ISPN000128: Infinispan version: Red Hat Data Grid 'Turia' 10.1.5.Final-redhat-00001
Jun 22, 2020 6:54:25 PM org.infinispan.lock.impl.ClusteredLockModuleLifecycle cacheManagerStarted
INFO: ISPN029009: Configuration is not clustered, clustered locks are disabled
key = value
----

=== Exercise 3: Functional API
The approach taken by the Functional Map API when working with multiple keys is to provide a lazy, pull-style API. All multi-key operations take a collection parameter which indicates the keys to work with (and sometimes contains 'value' information too), and a function to execute for each key/value pair. Each function's ability depends on the entry view received as a function parameter, which changes depending on the underlying map: `ReadEntryView` for `ReadOnlyMap`, `WriteEntryView` for `WriteOnlyMap`, or `ReadWriteView` for `ReadWriteMap`. The return type for all multi-key operations, except the ones from `WriteOnlyMap`, return an instance of `Traversable`, which exposes methods for working with the returned data from each function execution. Let's see an example:

- This example demonstrates some of the key aspects of working with multiple entries using the Functional Map API
- All data-handling methods (including multi-key methods) for `WriteOnlyMap` return `CompletableFuture<Void>`, because there's nothing the function can provide that could not be computed in advance or outside the function.

There is a particular type of multi-key operations which work on all keys/entries stored in Infinispan. The behavior is very similar to the multi-key operations shown above, with the exception that they do not take a collection of keys (or values) as parameters.

There are a few interesting things to note about working with all entries using the Functional Map API:

- When working with all entries, the order of the `Traversable` is not guaranteed
- Read-only's `keys()` and `entries()` offer the possibility to traverse all keys and entries present in the cache
-- When traversing entries, both keys and values, including metadata, are available
--- Contrary to Java's `ConcurrentMap`, there's no possibility to navigate only the values (and metadata) since there's little to be gained from such a method
--- Once a key's entry has been retrieved, there's no extra cost to provide the key as well.


Let us start by initializing our cache with the `DefaultCacheManager` as we have done so in the previous labs. However, we use the functional API, and hence after getting the cache, our `Map` implementation is different. 

==== How to use the Functional API? 
Using an asynchronous API, all methods that return a single result return a `CompletableFuture` which wraps the result. To avoid blocking, it offers the possibility to receive callbacks when the `CompletableFuture` has completed, or it can be chained or composed with other `CompletableFuture` instances. You do not need to write the following snippet, it should already be there. Let's get started with Exercise3.java.

[NOTE]
====
Please remove the following lines in the main method. 

      /* UNCOMMENT When starting this exercise
      UNCOMMENT When starting this exercise */
====


[source, java, role="copypaste"]
----
        DefaultCacheManager cacheManager = new DefaultCacheManager();
        cacheManager.defineConfiguration("local", new ConfigurationBuilder().build());
        AdvancedCache<String, String> cache = cacheManager.<String, String>getCache("local").getAdvancedCache();
        FunctionalMapImpl<String, String> functionalMap = FunctionalMapImpl.create(cache);
        FunctionalMap.WriteOnlyMap<String, String> writeOnlyMap = WriteOnlyMapImpl.create(functionalMap);<1>
        FunctionalMap.ReadOnlyMap<String, String> readOnlyMap = ReadOnlyMapImpl.create(functionalMap);
----

Next, what you would want to do is asynchronously write to this cache. Copy and paste the following snippet to Exercise3.java

[source, java, role="copypaste"]
----
        // TODO Execute two parallel write-only operation to store key/value pairs
        CompletableFuture<Void> writeFuture1 = writeOnlyMap.eval("key1", "value1",
                (v, writeView) -> writeView.set(v)); <1>
        CompletableFuture<Void> writeFuture2 = writeOnlyMap.eval("key2", "value2",
                (v, writeView) -> writeView.set(v));
----

<1> Write-only operations require locks to be acquired. They do not require reading previous value or metadata parameter information associated with the cached entry, which sometimes can be expensive since they involve talking to a remote node in the cluster or the persistence layer. Exposing write-only operations makes it easy to take advantage of this vital optimization.



And now lets do a read operation in similar 
[source, java, role="copypaste"]
----
        //TODO When each write-only operation completes, execute a read-only operation to retrieve the value
        CompletableFuture<String> readFuture1 =
                writeFuture1.thenCompose(r -> readOnlyMap.eval("key1", EntryView.ReadEntryView::get)); <1>
        CompletableFuture<String> readFuture2 =
                writeFuture2.thenCompose(r -> readOnlyMap.eval("key2", EntryView.ReadEntryView::get));
----
<1> Exposes read-only operations that can be executed against the functional map. The information that can be read per entry in the functional map. Read-only operations have the advantage that no locks are acquired for the duration of the operation.

Finally, let's print the operation as it completes.

[source, java, role="copypaste"]
----    
        //TODO When the read-only operation completes, print it out
        System.out.printf("Created entries: %n");
        CompletableFuture<Void> end = readFuture1.thenAcceptBoth(readFuture2, (v1, v2) -> System.out.printf("key1 = %s%nkey2 = %s%n", v1, v2));

        // Wait for this read/write combination to finish
        end.get();
----

So we have seen how a `WriteOnly` and `ReadOnly` `Map` works, let's also add the `ReadWriteMap`. Read-write operations offer the possibility of writing values or metadata parameters and returning previously stored information. Read-write operations are also crucial for implementing conditional, compare-and-swap (CAS) operations. Locks need to be acquired before executing the read-write lambda. 

[source, java, role="copypaste"]
----
        // Use read-write multi-key based operation to write new values
        // together with lifespan and return previous values
        Map<String, String> data = new HashMap<>();
        data.put("key1", "newValue1");
        data.put("key2", "newValue2");
        Traversable<String> previousValues = readWriteMap.evalMany(data, (v, readWriteView) -> {
            String prev = readWriteView.find().orElse(null);
            readWriteView.set(v, new MetaLifespan(Duration.ofHours(1).toMillis()));
            return prev;
        });
----

Now let's run our code and see how it works.

Run the above exercise as follows in the CodeReady terminal, or you can also choose to execute the command `Exercise3` in your MyWorkspace Menu on the right
[source, shell, role="copypaste"]
----
mvn clean compile && \
mvn exec:java -Dexec.mainClass=org.acme.Exercise3
----

You should be able to see an output similar to the following. On the last line you can see your key, value printed.
[source, shell, role="copypaste"]
----
Jun 22, 2020 6:59:09 PM org.infinispan.factories.GlobalComponentRegistry preStart
INFO: ISPN000128: Infinispan version: Red Hat Data Grid 'Turia' 10.1.5.Final-redhat-00001
Jun 22, 2020 6:59:09 PM org.infinispan.lock.impl.ClusteredLockModuleLifecycle cacheManagerStarted
INFO: ISPN029009: Configuration is not clustered, clustered locks are disabled
Created entries: 
key1 = value1
key2 = value2
Updated entries: 
ReadOnlySnapshotView{key=key1, value=newValue1, metadata=MetaParamsInternalMetadata{params=MetaParams{length=1, metas=[MetaLifespan=3600000]}}}
ReadOnlySnapshotView{key=key2, value=newValue2, metadata=MetaParamsInternalMetadata{params=MetaParams{length=1, metas=[MetaLifespan=3600000]}}}
Previous entry values: 
value1
value2
----

=== Exercise 4: Streaming data from the cache

Infinispan Distributed Java Streams can be used to calculate analytics over existing data. Through the overloading of methods, Infinispan can offer a simple way of passing lambdas that are `Serializable` without the need for explicit casting. Being able to produce binary formats for the lambdas is an essential step for java streams executions to be distributed. 

[NOTE]
====
Please remove the following lines in the main method. 

      /* UNCOMMENT When starting this exercise
      UNCOMMENT When starting this exercise */
====

With the following, we create a lambda to write data into our cache

[source, java, role="copypaste"]
----
        // TODO: Store some values
        int range = 10;
        IntStream.range(0, range)
                .boxed()
                .forEach(i -> cache.put(i + "-key", i + "-value"));
----

And now we read that data summing up the values.
[source, java, role="copypaste"]
----
        // TODO: Map and reduce the keys
        int result = cache.keySet().stream()
                .map(e -> Integer.valueOf(e.substring(0, e.indexOf("-"))))
                .collect(() -> Collectors.summingInt(Integer::intValue));
----

Now let's run our code and see how it works.

Run the above exercise as follows in the CodeReady terminal, or you can also choose to execute the command `Exercise4` in your MyWorkspace Menu on the right
[source, shell, role="copypaste"]
----
mvn clean compile && \
mvn exec:java -Dexec.mainClass=org.acme.Exercise4
----

You should be able to see an output similar to the following. On the last line, you can see your key, the value printed.
[source, shell, role="copypaste"]
----
Jun 22, 2020 7:00:04 PM org.infinispan.factories.GlobalComponentRegistry preStart
INFO: ISPN000128: Infinispan version: Red Hat Data Grid 'Turia' 10.1.5.Final-redhat-00001
Jun 22, 2020 7:00:05 PM org.infinispan.lock.impl.ClusteredLockModuleLifecycle cacheManagerStarted
INFO: ISPN029009: Configuration is not clustered, clustered locks are disabled
Result = 45
----

=== Exercise 5: Using Transactions

Transactions are essential in any business application. Usually, the transaction is used with the dataset, and quite often related to a database. Still, thats not exactly right, if you have a distributed dataset, one needs transactions for business logic to prevail. Infinspan provides transactions. There can be a scenario in which the cluster adds a node or entry has been written by another node. The Infinispan transaction manager is aware of such events and handles them. You can read more about the design of transactions here: https://github.com/infinispan/infinispan-designs

[NOTE]
====
Please remove the following lines in the main method. 

      /* UNCOMMENT When starting this exercise
      UNCOMMENT When starting this exercise */
====

Lets get the TransactionManager from the cache
[source, java, role="copypaste"]
----
        //TODO Obtain the transaction manager
        TransactionManager transactionManager = cache.getAdvancedCache().getTransactionManager();
----

We begin our transaction, write two entries, and then close it.

[source, java, role="copypaste"]
----
        // TODO Perform some operations within a transaction and commit it
        transactionManager.begin();
        cache.put("key1", "value1");
        cache.put("key2", "value2");
        transactionManager.commit();
----

Let's also do a rollback scenario. So we write to entries and rollback.

[source, java, role="copypaste"]
----
        //TODO Perform some operations within a transaction and roll it back
        transactionManager.begin();
        cache.put("key1", "value3");
        cache.put("key2", "value4");
        transactionManager.rollback();
----

Now let's run our code and see how it works.

Run the above exercise as follows in the CodeReady terminal, or you can also choose to execute the command `Exercise5` in your MyWorkspace Menu on the right
[source, shell, role="copypaste"]
----
mvn clean compile && \
mvn exec:java -Dexec.mainClass=org.acme.Exercise5
----

You should be able to see an output similar to the following. On the last line you can see your key, value printed.
[source, shell, role="copypaste"]
----
Jun 22, 2020 7:01:31 PM org.infinispan.factories.GlobalComponentRegistry preStart
INFO: ISPN000128: Infinispan version: Red Hat Data Grid 'Turia' 10.1.5.Final-redhat-00001
Jun 22, 2020 7:01:31 PM org.infinispan.lock.impl.ClusteredLockModuleLifecycle cacheManagerStarted
INFO: ISPN029009: Configuration is not clustered, clustered locks are disabled
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.
Jun 22, 2020 7:01:31 PM org.infinispan.transaction.lookup.GenericTransactionManagerLookup useDummyTM
INFO: ISPN000104: Using EmbeddedTransactionManager
key1 = value1
key2 = value2
key1 = value1
key2 = value2
----

So as you can see, even though we wrote the new values, but by rolling back, they do not exist anymore. 

=== Exercise 6: Queries to the Cache with Lucene

Infinispan includes a highly scalable distributed Apache Lucene Directory implementation.

This directory closely mimicks the same semantics of the traditional filesystem and RAM-based directories, being able to work as a drop-in replacement for existing applications using Lucene and providing reliable index sharing and other features of Infinispan like node auto-discovery, automatic failover, and rebalancing, optionally transactions, and can be backed by traditional storage solutions as filesystem, databases or cloud store engines.

The implementation extends Luceneâ€™s `org.apache.lucene.store.Directory` so it can be used to store the index in a cluster-wide shared memory, making it easy to distribute the index. Compared to rsync-based replication, this solution is suited for use cases in which your application makes frequent changes to the index, and you need them to be quickly distributed to all nodes. Consistency levels, synchronicity, and guarantees, total elasticity, and auto-discovery are all configurable; also, changes applied to the index can optionally participate in a JTA transaction, optionally supporting XA transactions with recovery.

[NOTE]
====
Please remove the following lines in the main method. 

      /* UNCOMMENT When starting this exercise
      UNCOMMENT When starting this exercise */
====

Since Lucene is part of Infinispan, We need to make sure that we have the right configuration for it. 

[source, java]
----
        // Create cache config
        ConfigurationBuilder builder = new ConfigurationBuilder();
        builder.indexing().index(Index.ALL) <1>
                .addProperty("default.directory_provider", "ram") <2> 
                .addProperty("lucene_version", "LUCENE_CURRENT"); <3> 

        // Obtain the cache
        Cache<String, Person> cache = cacheManager.administration()
                .withFlags(CacheContainerAdmin.AdminFlag.VOLATILE)
                .getOrCreateCache("cache", builder.build());
----

<1> Here we are telling our Cache config that we want to index all entries
<2> The storage for Lucene is in the memory
<3> and we want to give it a version

Now, let's add a bit of more code to the above example. 
In the following code, we get the QueryFactory and create a query. 

[source, java, role="copypaste"]
----
        // TODO: Obtain a query factory for the cache
        QueryFactory queryFactory = Search.getQueryFactory(cache);
        // Construct a query
        Query query = queryFactory
            .from(Person.class)
            .having("name")
            .eq("William")
            .toBuilder()
            .build();
        // Execute the query
        List<Person> matches = query.list();

----

Now let's run our code and see how it works.

Run the above exercise as follows in the CodeReady terminal, or you can also choose to execute the command `Exercise6` in your MyWorkspace Menu on the right
[source, shell, role="copypaste"]
----
mvn clean compile && \
mvn exec:java -Dexec.mainClass=org.acme.Exercise6
----

You should be able to see an output similar to the following. On the last line you can see your key, value printed.
[source, shell, role="copypaste"]
----
Jun 22, 2020 7:03:18 PM org.hibernate.search.engine.Version <clinit>
INFO: HSEARCH000034: Hibernate Search 5.10.7.Final-redhat-00001
Jun 22, 2020 7:03:18 PM org.hibernate.annotations.common.reflection.java.JavaReflectionManager <clinit>
INFO: HCANN000001: Hibernate Commons Annotations {5.0.5.Final}
Jun 22, 2020 7:03:18 PM org.infinispan.query.impl.LifecycleManager createQueryInterceptorIfNeeded
INFO: ISPN014003: Registering Query interceptor for cache cache
Match: Person [name=William, surname=Wordsworth]Match: Person [name=William, surname=Shakespeare]
----                

It's quite simple to add Lucene based search to your cache. Try to change the parameters a bit and experience this more. 

==== Congratulations!

You have completed the first introductory exercises to Red Hat Data Grid 8.0. 

1. *Exercise 1* - Creating a Cache
2. *Exercise 2* - JSR-107, JCache
3. *Exercise 3* - Functional API
4. *Exercise 4* - Streaming data from the Cache
5. *Exercise 5* - Using Transactions
6. *Exercise 6* - Queries to the Cache with Lucene

You should now be able to create caches, stream data and so much more. 
Let's move on to the next section and experience more in-depth examples.

