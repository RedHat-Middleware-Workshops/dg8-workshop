
= Remote Cache
:experimental:

=== HotRod Client
Hot Rod is a binary TCP protocol that Infinispan offers for high-performance client-server interactions with the following capabilities:

- Load balancing
-- Hot Rod clients can send requests across Infinispan clusters using different strategies to increase reliability
- Failover
-- Hot Rod clients can monitor Infinispan cluster topology changes and automatically switch to available nodes to ensure requests never go to offline nodes
- Efficient data location
-- Hot Rod clients can find key owners and make requests directly to those nodes, which can help reduce latency

=== RemoteCache
A `RemoteCache`, as the name suggests, is a cache that can be accessed remotely. The Data Grid server will host this remote cache, and clients will connect to this remote cache.
From a design perspective it gives more flexibility and also allows to have central deployments with multiple caches residing in it; there by making management and operations a bit more simpler. 

There are some semantic differences how Infinispan/Red Hat Data Grid expose `RemoteCache` vs `EmbeddedCache`. The collection methods `keySet`, `entrySet`, and `values` are backed by the remote cache. Every method called is sent back into the `RemoteCache`. This is useful as it allows for the various keys, entries or values to be retrieved lazily, and not requiring them all be stored in the client memory at once if the user does not want. These collections adhere to the `Map` specification, noting that `add` and `addAll` are not supported but all other methods are supported. One thing to note is the `Iterator.remove`, `Set.remove`, or `Collection.remove` methods require more than 1 round trip to the server to operate. You can check out the `RemoteCache` Javadoc to see more details about these and the other methods.


=== Project details
For this example we are going to create a simple web application. it will take some input from a web form and then add the entries into the Cache. 
However in this case we will be using the `RemoteCacheManager` and we will be using ProtoStream API. All of this with a Quarkus based application. 

So let's get cracking. But first let's take a look at the project.

Back in {{ CHE_URL }}[CodeReady Workspaces^], navigate to the project `dg8-quarkus-client-example`. This is a template project, and you will be writing code into this project.
As you can see there is already some files in place. Let's take a look into what these files are and do.


=== The Maven dependencies
Open the `pom.xml` file in the project.

We will be using the following dependencies to create our service

[source, xml]
----
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-resteasy</artifactId> <1>
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-resteasy-jsonb</artifactId> <2> 
    </dependency>
    <dependency>
      <groupId>io.quarkus</groupId>
      <artifactId>quarkus-infinispan-client</artifactId> <3>
    </dependency>
    <dependency>
----

<1> `quarkus-resteasy` for our REST endpoint
<2> `quarkus-resteasy-jsonb` for Json serialization for our REST endpoint
<3> `quarkus-infinispan-client` for enabling us to use Data Grid's `RemoteCache`


=== Protobuf
Protobuf or Protocol Buffers are a method of serializing structured data. Protocol buffers are a flexible, efficient, and automated mechanism for serializing structured data. You can easily write and read your structured data to and from a variety of data streams and using a variety of languages. Protobuf is all about structured data, so the first thing to do is to define the structure of your data. This is accomplished by declaring Protobuf message types in .proto files. 

In our example the `game.proto` file looks like this:

[source, protobuf, role="copypaste"]
----
package quickstart; <1> 

message Game { <2>
    required string name = 2; <3>
    required string description = 3; <4>
}
----

<1> We define a package for our message
<2> We define a name for our message. A message is similar to an entity. 
<3> We specify that our message has a string attribute called `name` and that it is required
<4> We also specify that our message has a string attribute called `description` and that it is required.

Save the above content in the following file: `src/main/resources/META-INF/game.proto`

=== Marshallers
As described in the previous section, a fundamental concept of the Protobuf format is the definition of messages in the .proto schema to determine how an entity is represented. However, in order for our Java applications to utilize the Protobuf format to transmit/store data, it’s necessary for our Java objects to be encoded. This is handled by the ProtoStream library and its configured Marshaller implementations, which convert plain old Java objects into the Protobuf format.

Although generating resources is the easiest and most performant way to utilize ProtoStream, this method might not always be viable. For example, if you are not able to modify the Java object classes to add the required annotations. For such use cases, it’s possible to manually define the .proto schema and create a manual marshaller implementation. Let's define our Marshaller.

Open the `GameMarshaller` class in the `dg8-quarkus-client-example/src/main/java/org/acme/rest/json` folder.

Add the following method to our `GameMarshaller` class. In the following code we specify how we are going to read from our ProtoStream. We could add any additional processing on the stream if we wanted to. For now we take a simplified read and return a `Game` object. Hence every time a stream is read from the Cache, this method will be called.

[source, java, role="copypaste"]
----
    @Override
    public Game readFrom(MessageMarshaller.ProtoStreamReader reader) throws IOException {
        String name = reader.readString("name");
        String description = reader.readString("description");
        return new Game(name, description);
    }
----

Next we can also define a writer method. It takes a Game object and translates that into a stream.

[source, java, role="copypaste"]
----
    @Override
    public void writeTo(MessageMarshaller.ProtoStreamWriter writer, Game game) throws IOException {
        writer.writeString("name", game.getName());
        writer.writeString("description", game.getDescription());
    }
----

Let's specify which class handles our Stream data. 

[source, java, role="copypaste"]
----
    @Override
    public Class<? extends Game> getJavaClass() {
        return Game.class;
    }
----

And finally here we let the Serialization process know what type we are doing this for. i.e. packagename.Class

[source, java, role="copypaste"]
----
    @Override
    public String getTypeName() {
        return "quickstart.Game";
    }
----

Perfect we have our Marshaller configured.

=== Configuring our RemoteCache
Let's move on and create our RemoteCache configuration

For this open the `Init.java` and add the following member variables to it. 

[source, java, role="copypaste"]
----
    public static final String GAME_CACHE = "games"; <1>

    @Inject
    RemoteCacheManager cacheManager; <2> 

    private static final String CACHE_CONFIG = "<distributed-cache name=\"%s\">" <3>
          + " <encoding media-type=\"application/x-protostream\"/>" <4>
          + "</distributed-cache>";
----

<1> First we specify a class level variable which is the name of our Cache 
<2> We inject the `cacheManager` to our file. We only want to load the `CacheManager` once, and since its a heavy object, we want to do it at startup.
<3> In addition to defining cache configuration within code, we can also configure a cache with xml. We are doing that here just to show that it is possible. We could have also loaded this from a file in the `META-INF` directory, but for a short demo this works okay as well.
<4> The encoding of the cache is Protostream to store encoded data in Protobuf in the cache and get the best interoperability and Query support.

[source, java, role="copypaste"]
----
    void onStart(@Observes @Priority(value = 1) StartupEvent ev) {
        String xml = String.format(CACHE_CONFIG, "games"); <1>
        cacheManager.administration().getOrCreateCache(GAME_CACHE, new XMLStringConfiguration(xml)); <2>
    }
----

You might remember the `onStart` method from our previous labs. We are doing the same thing here. 
<1> We use the xml defined in a `String` and pass it on to the Red Hat Data Grid server to parse it and create a new cache called `games`
<2> We then ask the `cacheManager` to get the Cache for us or create a new one if it doesn't exist

Now we should have a `RemoteCacheManager` configured, all we need to do now is to inject it in our REST resource.

=== REST endpoint

Open up the `GameResource.java` class. This class uses JAX-RS to define REST resources for our application.

In the following code we inject our `RemoteCache`, and we specify which remote cache we want by passing the variable `GAME_CACHE` to it, which we have initialized previously in our `Init` class.

Add this code to the `GameResource.java`

[source, java, role="copypaste"]
----
    @Inject
    @Remote(GAME_CACHE)
    RemoteCache<String, Game> gameStore;
----


The following are two simple GET and POST method implementation. 

[source, java, role="copypaste"]
----
    @GET
    public Set<Game> list() {
        return new HashSet<>(gameStore.values());
    }

    @POST
    public Set<String> add(Game game) {
        gameStore.putAsync(game.getName(), game);
        return gameStore.keySet();
    }
----

<1> The `list` method is simply returning the games back to the front-end
<2> The `add` method is using the Async api of Infinispan/Red Hat Data Grid to add the entry into the cache

Perfect. We are all set to deploy our application to Openshift and see how the `RemoteCache` will work.

=== Deploying to Openshift and scaling

Let's prepare to deploy the application to Openshift

For this open up the `application.properties file` located in `src/main/resources/application.properties`

[source, properties, role="copypaste"]
----

quarkus.infinispan-client.server-list=datagrid-service:11222 <1>

# Auth
quarkus.infinispan-client.use-auth=true <2>
quarkus.infinispan-client.auth-username=developer <3> 
quarkus.infinispan-client.auth-password=<4>

quarkus.infinispan-client.auth-server-name=infinispan <5>
quarkus.infinispan-client.auth-realm=default <6>
quarkus.infinispan-client.sasl-mechanism=SCRAM-SHA-512 <7>

quarkus.infinispan-client.trust-store=/var/run/secrets/kubernetes.io/serviceaccount/service-ca.crt <8>
quarkus.infinispan-client.trust-store-type=pem <9>



# Openshift extension settings.
quarkus.openshift.expose=true 
quarkus.kubernetes-client.trust-certs=true
quarkus.http.cors=true
quarkus.container-image.build=true
quarkus.kubernetes.deploy=true


----

<1> Sets the Infinispan hostname/port to connect to. Each one is separated by a semicolon (eg. host1:11222;host2:11222)
<2> boolean for denoting that autheticaiton is on
<3> Sets username used by authentication, in our case its developer, thats the default from the operator.
<4> Sets password used by authentication, we do not have this yet. we will find it out from the secrets. 
<5> The authentication server name
<6> the default security realm that enables authentication on the cache server
<7> Security Mechanism used, SASL, MD-5 etc.
<8> The trust store for our certificate
<9> And finally the trust store type

Let's go fill that password field in the above properties file.

Run the following command on the terminal and the passwords will be shown. Copy the password belonging to the `developer` user and add it to the password field `quarkus.infinispan-client.auth-password=`. 

[source, shell, role="copypaste"]
----
oc get secret datagrid-service-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode
----

Let's go ahead and deploy the application to OpenShift. 

[source, shell, role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/dg8-workshop-labs/dg8-quarkus-client-example
----

Let's wait for this build to be successful! 

Now navigate to the link:{{ CONSOLE_URL }}[OpenShift web console^] and switch to the topology view

image::gameserviceocp.png[cdw, 700, align="center"]

Find the `dg8-quarkus-client-example` application and click on the route to navigate to the application

image::gameserviceocproute.png[cdw, 700, align="center"]

image::gamerestservice.png[cdw, 700, align="center"]

Try playing around with the application and adding some games.

=== Enabling Near Cache
Near caches are optional caches for Hot Rod Java client implementations that keep recently accessed data close to the user, providing faster access to data that is accessed frequently. This cache acts as a local Hot Rod client cache that is updated whenever a remote entry is retrieved via `get` or `getVersioned` operations.

In Red Hat Data Grid, near cache consistency is achieved by using remote events, which send notifications to clients when entries are modified or removed (refer to Remote Event Listeners). With near caching, local cache remains consistent with remote cache. Local entry is updated or invalidated whenever remote entry on the server is updated or removed. At the client level, near caching is configurable as either of the following:

- *DISABLED* - the default mode, indicating that near caching is not enabled
- *INVALIDATED* - enables near caching, keeping it in sync with the remote cache via invalidation messages.

image::nearcache.png[Near Caching, 700]

==== When should I use it? 
Near caching can improve the performance of an application when most of the accesses to a given cache are read-only and the accessed dataset is relatively small. When an application is doing lots of writes to a cache, invalidations, evictions, and updates to the near cache need to happen. In such a scenario the benefits a near cache provides won't necessarily be beneficial.

For Quarkus, near caching is disabled by default You can enable it by setting the profile config property `quarkus.infinispan-client.near-cache-max-entries` to a value greater than `0`. You can also configure a regular expression so that only a subset of caches have near caching applied through the `quarkus.infinispan-client.near-cache-name-pattern` property.


Add the following properties to our `application.properties` to enable near caching.

[source, properties, role="copypaste"]
----
quarkus.infinispan-client.near-cache-max-entries=40
quarkus.infinispan-client.near-cache-name-pattern=*i8n-.
----

Let's go ahead and re-deploy the application to OpenShift. 

[source, shell, role="copypaste"]
----
mvn clean package -DskipTests -f $CHE_PROJECTS_ROOT/dg8-workshop-labs/dg8-quarkus-client-example
----

You should see a Build Successful message from this run as well. 

Notice that any entries that you might have added to the cache prior to this deployment are still there. That wasn't the case in the embedded cache, since we were not using any stores and every time the application started the cache was empty. But in this case since the cache is remote, you will still see the entries from last time. Its important to note that there are different ways you can configure and setup the cache. For more details visit the Documentation pages for Red Hat Data Grid.


=== Caching with Hibernate and JPA and Quarkus

When using Hibernate ORM in Quarkus, you don’t need to have a `persistence.xml` file for configuration. Using such a classic configuration file is an option, but unnecessary unless you have specific advanced needs. Let's see first how Hibernate ORM can be configured without a persistence.xml resource.

In Quarkus, you just need to

- add your configuration settings in `application.properties`
- annotate your entities with `@Entity` and any other mapping annotation as usual

Other configuration needs have been automated: Quarkus will make some opinionated choices and educated guesses. 

[source, java, role="copypaste"]
----
package org.acme;

@Entity
@Cacheable
public class Country {
    // ...

    @OneToMany
    @Cache(usage = CacheConcurrencyStrategy.READ_ONLY)
    List<City> cities;

    // ...
}
----

In the above code, just using the `@Cacheable` annotation will make sure that Infinispan is used as the second level cache for the entities. You also don’t need to pick an implementation. A suitable implementation based on technologies Infinispan is included as a transitive dependency of the Hibernate ORM extension, and automatically integrated during the build.

=== Recap

. You learnt about `RemoteCache` and HotRod client
. You learnt about Protostream and marshallers in Infinispan
. You deployed your Quarkus app using `RemoteCache` to OpenShift
. You learnt about near caching and its use case
. And finally we sum it up with JPA and Second Level Cache

*Congratulations!!* you have completed the this lab on RemoteCache. Let's move to the next lab and learn how we can use the new REST API in DataGrid to our advantage.
