== Embeded Cache with a cluster
:experimental:

In this lab we are going to cluster the embedded cache. Inifinispan does this very nicely. We do not need to change a lot of configuration to achieve this.
After configuration, we will deploy our application to Openshift and see how the cluster will work in a cloud environment.
Following diagram illustrates the topology we want to achieve. Although clustering cache nodes can be in 100s, we will start with a couple of them and see how we can get the feature set we need for a cache.

image::clusteredembeddedcache.png[Caching, 700]

=== Clustering

Open the ScoreService.java again in our project `dg8-embedded-quarkus`

We are going to add the following lines of code to our onStart method

[source, java, role="copypaste"]
----
        GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder();
        global.transport().clusterName("ScoreCard");
        cacheManager = new DefaultCacheManager(global.build());
----

Replace the onStart method in our ScoreService.java

[source, java, role="copypaste"]
----
    void onStart(@Observes @Priority(value = 1) StartupEvent ev){
        GlobalConfigurationBuilder global = GlobalConfigurationBuilder.defaultClusteredBuilder(); // <1>
        global.transport().clusterName("ScoreCard").addProperty("configurationFile", "default-configs/jgroups-kubernetes.xml")
            .serialization().addContextInitializer(new org.acme.ScoreInitializerImpl()); // <2>
        cacheManager = new DefaultCacheManager(global.build());

        ConfigurationBuilder config = new ConfigurationBuilder();

        config.expiration().lifespan(5, TimeUnit.MINUTES)
                .clustering().cacheMode(CacheMode.REPL_SYNC); // <3>

        cacheManager.defineConfiguration("scoreboard", config.build());
        scoreCache = cacheManager.getCache("scoreboard");
        scoreCache.addListener(new CacheListener());


        log.info("Cache initialized");

    }
----
<1> We define a global configuration, since we are going to be in a clustered mode, hence everytime a new instance of our app will be created it will have the global configuration parameters to talk to the other nodes if they are present.

<2> Infinispan nodes rely on a transport layer to join and leave clusters as well as to replicate data across the network. Infinispan uses JGroups technology to handle cluster transport. You configure cluster transport with JGroups stacks using DNS_PING on OpenShift, which define in _default-configs/jgroups-kubernetes.xml_. Here we have defined a unique clusterName for our app's embedded cache manager.

<3> We setup a distributed, replicated cache, which means that all our nodes will have all the keys in its memory. But each node will return the correct values for all the keys.

We will take a look into replicaiton and distribution furhter in this lab.

Also open the CacheListener and make sure that the `@Listener` class annotation is changed to:

[source, java, role="copypaste"]
----
@Listener(primaryOnly = false)
----

This will ensure all nodes get notified on changes.

It is possible in a non transactional cache to receive duplicate events. This is possible when the primary owner of a key goes down while trying to perform a write operation such as a put. Infinispan internally will rectify the put operation by sending it to the new primary owner for the given key automatically, however there are no guarantees in regards to if the write was first replicated to backups. Thus more than 1 of the following write events (CacheEntryCreatedEvent, CacheEntryModifiedEvent & CacheEntryRemovedEvent) may be sent on a single operation.

[NOTE]
====
It is recommended that at this moment you press KBD:[CTRL+C] in the `Start Live Coding` terminal to quit our app, then close all terminal windows that you might have opened in the previous labs. to keep a clear view of our lab
====

First lets login to Openshift. You will find the button in the right corner in MyWorkspace menu.
Click `Login to Openshift` and login with:

* **Username**: `{{USER_ID}}`
* **Password**: `{{ OPENSHIFT_USER_PASSWORD }}`

[NOTE]
====
After you log in using **Login to OpenShift**, the terminal is no longer usable as a regular terminal. You can close the terminal window. You will still be logged in when you open more terminals later!
====


Next, run the following command to add the Openshift extension for Quarkus (Be sure you're in the right directory: `cd /projects/dg8-workshop-labs/dg8-embedded-quarkus`).

The Openshift extension makes it easy to deploy your application to openshift, rather then taking all the different steps from an oc command line, you can do that through your maven build.

run the following in your terminal, you should see a BUILD SUCCESSFUL message when done.
[source, shell, role="copypaste"]
----
mvn quarkus:add-extension -Dextensions="openshift"
----

Now open the application.properties file in `src/main/resources/application.properties`

Add the following properties to it

[source, shell, role="copypaste"]
----
quarkus.http.cors=true
quarkus.openshift.expose=true <1>

# if you dont set this and dont have a valid cert the deployment wont happen
quarkus.kubernetes-client.trust-certs=true <2>
----

<1> The first property makes sure that once our application is deployed it will expose a route
<2> The second property makes sure that incase you dont have valid certificates the build wont stop. in our case that can likely be the case since its not a production environment rather a demo one.

Perfect everything is in order. Make sure you are logged into openshift. You can run the following command in your terminal to confirm:
[source, shell, role="copypaste"]
----
oc whoami
----

The command should return your user name: {{ USER_ID }}, if you are logged in.

Let's first generate a container image for our application

[source, shell, role="copypaste"]
----
mvn clean package -Dquarkus.container-image.build=true
----

You should see a build successful message at the end. That mean everything worked out.

Now lets deploy our application to Openshift

[source, shell, role="copypaste"]
----
mvn clean package -Dquarkus.kubernetes.deploy=true
----

Also remmember next time we need to deploy we just need to run the above deploy command again. thats all!

Lets wait for this build to be successfull!

=== Openshift Console
First, open a new browser with the link:{{ CONSOLE_URL }}[OpenShift web console^]

image::openshift_login.png[openshift_login, 700]

Login using:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

You should see something as follows:

image::openshiftprojectview.png[Caching, 900]


Click on the project name and you should see something similar:

image::lab2ocpoverview.png[Caching, 900]

Create a new _JGroup_ service to find members among Infinispan servers. Press the plus sign on the right top corner as shown in the picture:

image::plussigntop_ocpconsole.png[Run yaml in console, 700]

Paste the below Service YAML code into the editor and click **Create**:

[source, yaml, role="copypaste"]
----
kind: Service
apiVersion: v1
metadata:
  name: jcache-quarkus-ping
  namespace: {{ USER_ID }}-cache
spec:
  ports:
    - name: ping
      protocol: TCP
      port: 7800
      targetPort: 7800
  selector:
    app.kubernetes.io/name: jcache-quarkus
  clusterIP: None
  type: ClusterIP
----

Let's patch _DeploymentConfig_ to add the above target port to the embedded cache application. Go back to CRW terminal window and execute the following commands:

[source, sh, role="copypaste"]
----
oc patch dc/jcache-quarkus -p '{"spec": {"template": {"spec": {"containers": [{"name": "jcache-quarkus","ports":  [{"name": "http","containerPort": 8080,"protocol": "TCP"},{"name": "ping","containerPort": 7800,"protocol": "TCP"}]}]}}}}' && oc rollout latest dc/jcache-quarkus && oc rollout status -w dc/jcache-quarkus
----

Navigate to _Workloads_ tab, then click on the `jcache-quarkus` workload. Next, click on the _Resources_ tab on the right, and at the bottom you will see the route to your application. You can also click at the route and it will take you to the application page, same as we have done in the previous lab. if append /api to the url you will be on the api endpoint.

Now go back to the `Details` tab for the applicaiton and Click on the pod scaler and scale to 2 pods.

image::lab2podscaler.png[Caching, 900]

This will spin up another instance of the app, and cluster them together automatically.

Let's find out if 2 Inifinspan cache servers joined the cluster. Click on `View Logs` in _Resources_ tab:

image::viewLogs.png[Caching, 600]

You will see the following ISPN logs. It sometimes takes a min to join the cluster:

image::clusterlogs.png[Caching, 800]

Now open another terminal in CodeReady workspaces and change to the scripts directory

[source, shell, role="copypaste"]
----
cd dg8-embedded-quarkus/scripts
----

in this directory we have a load.sh file. Open this file in CodeReady Workspaces and change the variable `EP` to the application route from the browser (including the `/api` suffix):

image::lab2epchange.png[Caching, 900]

and then run load.sh

[source, shell, role="copypaste"]
----
./load.sh
----

Go back to the resrouce view of your application and then click view logs, you should see same Entry logs as follows in both nodes.

Node1:

image::distsyncnode1.png[Caching, 900]

Node2:

image::distsyncnode2.png[Caching, 900]

=== Design Considerations

Firstly, p2p deployments are simpler than client-server ones because in p2p, all peers are equals to each other and this simplifies deployment. If this is the first time you are using Infinispan, p2p is likely to be easier for you to get going compared to client-server.

Client-server Infinispan requests are likely to take longer compared to p2p requests, due to the serialization and network cost in remote calls. So, this is an important factor to take in account when designing your application. For example, with replicated Infinispan caches, it might be more performant to have lightweight HTTP clients connecting to a server side application that accesses Infinispan in p2p mode, rather than having more heavyweight client side apps talking to Infinispan in client-server mode, particularly if data size handled is rather large. With distributed caches, the difference might not be so big because even in p2p deployments, youâ€™re not guaranteed to have all data available locally.

Environments where application tier elasticity is not important, or where server side applications access state-transfer-disabled, replicated Infinispan cache instances are amongst scenarios where Infinispan p2p deployments can be more suited than client-server ones.

Congratulations we are at the end of this lab!

=== Recap
<1> You created our own Cache and learnt how to us EmbeddedCacheManager
<2> You learnt how to use ConfigurationBuilder and Configuration objects to define our Configurations for the Cache and CacheManager
<3> You learnt about how to create and Embedded Cluster
<4> You learnt how to deploy a Quarkus application with emebedded cache and scale it.
<5> You learnt the difference between Replicated and Distributed Cache and how clustering and listeners works.

*Congratulations!!* you have completed the second lab of this workshop. Lets move to the next lab and learn how we can create a remote cache and how it can benefit our applications.

