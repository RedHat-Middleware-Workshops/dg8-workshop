== DatGrid 8.0 in 10 minutes
The latest update to Red Hat Runtimes is with the release of Red Hat Data Grid 8.0, which provides a distributed in-memory, NoSQL datastore solution. Your applications can access, process, and analyze data at in-memory speed to deliver a superior user experience. Whether you are using legacy applications or a new breed of microservices and functions, Red Hat Data Grid 8.0 will enable the journey to Open Hybrid Cloud. Data Grid includes the Infinispan open-source software project. It is available to deploy as an embedded library, as a standalone server, or as a containerized application on Red Hat OpenShift Container Platform. 

image::dg8.png[Red Hat Data Grid 8.0, 900]


=== A Full Lifecycle Operator to reduce deployment and management overhead in OpenShift
An Operator enables the operations and lifecycle management for an application by using the underlying Kubernetes APIs. That means complex applications, e.g., consumed as services such as distributed caching, databases, etc. can easily get upgraded when newer versions arrive and more; also meaning no human intervention. The Operator SDK enables developers to write such Operators. Red Hat Data Grid 8.0 introduces a fully supported Data Grid Operator that provides operational intelligence.

With the latest release of Red Hat Data Grid, now you can also use it with an Operator. 

Try some of the following documentation to create a Data Grid Operator subscription and get running in OpenShift:
Getting Started with Data Grid Operator
Running Data Grid for OpenShift

=== A new server architecture 
Cloud and Container Native Data Grid needs to be a reduced footprint, and that's what the latest version of Red Hat Data Grid brings. It reduces both the disk footprint and initial heap size upto 50%, leaving more memory for your data. You can now run the server without the Red Hat JBoss Enterprise Application Platform (EAP), ensuring a lower memory and disk footprint
also simplifies configuration. 

Moreover, Data Grid 8.0 servers provide several enhancements and improvements to security, including integration with Red Hat SSO and a smaller attack surface.

See Getting Started with Data Grid Server and create a running Data Grid cluster in a few minutes.

=== A more performant and rich REST API 
Red Hat Data Grid 8.0 introduces REST API v2. 
The API has 50% faster response rates compared to v1. There are also new capabilities introduced such as
- Now you can access data and manipulate objects such as counters.
- Perform operations such as gracefully shutting down Data Grid clusters or transferring cache state to backup locations when using cross-site replication.
- And lastly, Monitor cluster and server health and retrieve statistics.
Moreover, Red Hat Data Grid REST API v2 also automatically converts between storage formats such as JSON, XML, Protobuf, and plain text for increased interoperability. The Red Hat Data Grid engineering team develop and maintain comprehensive REST API Documentation


=== A powerful CLI 
In 8.0, Data Grid gives you a new CLI with intuitive commands for remotely accessing data and managing clusters.
The CLI uses familiar Bash commands for navigating, such as cd and ls. It also provides command history and auto-completion for ease of use. 

Additionally, the CLI provides help text and man pages for commands with clear examples.

Try the docs: Getting Started with the Data Grid CLI.

=== Enhanced observability 
Now you can use the /metrics endpoint for integration with Prometheus. Moreover, Red Hat Data Grid 8 is also compatible with Eclipse Microprofile Metrics API. More specific metrics and gauges are included. For more details, check the documentation here. Data Grid 8.0 also offers improved statistics and management operations via JMX and updates to logging with coarse-grained logging categories and support for logs in JSON format.


== What is Caching and how to apply it with Red Hat Data Grid?

Modern applications use data and lets assume lots of data, whether it comes from databases, files, webservices, rpc call etc. 
When an application has to process this data, what is the most natural way of doing it? Mostly the application will process this data in memory. 
Let's assume I have a very slow database, this could be due to any reason, network latency, or even big queries that return alot of data. 
So the most straightforward way of handling this, would be to store some of that data in the memory. By doing so, you would be able to process requests to your systems much faster. However there are challenges. How much data should you store in memory? and most importantly what will happen in case of failure scenarios? 

- Will you lose all the state of your data in memory? 
- Will you need to re-read all your data and events inorder to get back to the same state where you failed. Or you might have to let go of that entirely. 

Now the above two might seem very simple, but that tasks can be tideous and most importantly error prone. 
So at this point we could introduce a local cache that e.g. could be a ConcurrentHashMap. and alot of us might have done this in the past. However as a developer you might know that this has not much effect in case of failures and the handling that you will need to do incase of failures. 
So the need is entirely for a component that can not just cache data in the memory, but give 

1. A consistent way to handle data and state in the memory. 
2. Resiliencey in case of failures. 
3. Processing efficency and performance.
4. Events, streams, and distribution capabilites. 


image::caching.png[Caching, 900]


By having such capabilites a cache is no longer just a datastructure in the memory, but also as a developer now you have the possiblity to take this component out of your local in memory processing and distribute it out on the network. Thereby incase of application failures you will still be able to access this data from the last point where you left off. 

Now getting back to our primary question, how much data should you store in memory? Partially we have already discussed this above. Whats important is that as a developer you should be able to specifiy TTL (Time To Live) for your cache and its entries. You should be able to define eviction and expiration. There by knowing when your cash is hot and what data resides in it. Most over you should be able to do this distributed, cluster wide or remotely. 

Once a cache is remote, we also want some of the distributed features, e.g. monitoring. 
Lets take a look at some of the caching strategies.

==== Local cache
The primary use for Red Hat Data Grid is to provide a fast in-memory cache of frequently accessed data. Suppose you have a slow data source (database, web service, text file, etc): you could load some or all of that data in memory so that itâ€™s just a memory access away from your code. Using Red Hat Data Grid is better than using a simple ConcurrentHashMap. By setting up an embedded cache, Red Hat Dat Grid also allows you to tap into more features e.g. expiration, eviction, events on the cache etc. All make out a much better way of handling your cache and component design. Moreover if you would want to cluster such a cache that is also easily possible. 

==== As a clustered cache
So lets assume you started with a local embedded cache in your application, and now you suddenly realize that one instance of your application is not enough to handle the load from your users or systems. What do you do? With Red Hat Data Grid you can now scale that cache into a cluster. You dont need to change how you use your cache, but adding a few additonal config params you can now have a clustered cache and there by having muliptle instances of your application listenting to the same coherent cache. Events will be fired accorss the clusters, your eviction and expiration will happen accorss the cluster. 
And most over, you now even have the possiblity to distribute your keys accross the cluster. Red Hat Data Grid can scale horizontally to hundreds of nodes. 

==== As a remote cache
Lets just say you used the clusterd cache, and embedded it in your application, which means that everytime a new instance of your application started you would have a new instance of your embedded cache ready to become part of the cluster. Now this is all great. But what if, you dont want that clustering in your application. rather then you might want to use a component outside of your applications lifecycle. Or you would want to share this cache accross multiple applications. In that case the Red Hat Data Grid could be used as a remote data grid. Now you can access your cache via multiple programming runtimes. e.g. Vert.x, Quarkus, NodeJS, C#, C/C++ etc. And your cache lifecycle will be independant of the applications life cycle, which is a great advantage in many cases. 


Congratulations! By now you understand the different patterns of caching, and the requirements. Lets go ahead and create our first application and learn how we can use Red Hat Data Grid to achieve caching. Press next! 


 
=== Additional Resources:
- Traditional zip deployments are available on the link:https://access.redhat.com[Customer Portal, window=_blank].
- The container distribution and operator are available in the link:https://catalog.redhat.com/software/containers/explore[Red Hat Container Catalog, window=_blank]
- Product documentation is available link:https://docs.redhat.com[here, window=_blank]
- Getting Started Guide that will get you running with RHDG 8 in 5 minutes.
- link:https://access.redhat.com/documentation/en-us/red_hat_data_grid/8.0/html/data_grid_migration_guide/index[Migration Guide, window=_blank] 
- link:https://github.com/redhat-developer/redhat-datagrid-tutorials[Starter Tutorials, window=_blank]
- link:https://access.redhat.com/articles/4933371[Supported Components, window=_blank]
- link:https://access.redhat.com/articles/4933551[Supported Configurations, window=_blank]

