=== Monitoring the Data Grid
:experimental:

==== Understanding the Stack
OpenShift Container Platform includes a pre-configured, pre-installed, and self-updating monitoring stack that provides monitoring for core platform components. OpenShift Container Platform delivers monitoring best practices out of the box. A set of alerts are included by default that immediately notifies cluster administrators about issues with a cluster. Default dashboards in the OpenShift Container Platform web console include visual representations of cluster metrics to help you quickly understand the state of your cluster.

The OpenShift Container Platform monitoring stack is based on the Prometheus open-source project and its wider ecosystem. The monitoring stack includes the following:

*Default platform monitoring components.* 

A set of platform monitoring components are installed in the `openshift-monitoring` project by default during an OpenShift Container Platform installation. This provides monitoring for core OpenShift Container Platform components including Kubernetes services. The default monitoring stack also enables remote health monitoring for clusters. These components are illustrated in the Installed by default section in the following diagram.

*Components for monitoring user-defined projects.* 

After optionally enabling monitoring for user-defined projects, additional monitoring components are installed in the `openshift-user-workload-monitoring` project. This provides monitoring for user-defined projects. These components are illustrated in the User section in the following diagram.

image::ocp-prometheus-arch.png[Monitoring - Topology, 700]

In our cluster, we have already enabled the user-workloads monitoring, which means that all our applications exposing `/metrics` can be scraped by Prometheus in all our user namespaces. It is also possible to have one's own Prometheus installation, hence a range of flexibility is available for different architecture and infrastructure practices.

Let's take a look at metrics in Openshift for our workloads and then also take a look at the metrics that are being scrapped via the Openshift web console. Data Grid exposes metrics that can be used by Prometheus and Grafana for monitoring and visualizing the cluster state.

In the OpenShift Web Console, select the *</> Developer* perspective. You should be able to see the `datagrid-service` as shown in the picture below. Click the service of the Data Grid cluster and in the tab on the right click `Observe`.

image::monitoring-dg-dev-view.png[Monitoring - Topology, 700]

Remember the full definition of the Infinispan CR that we created in a previous section:

[source, yaml]
----
apiVersion: infinispan.org/v1
kind: Infinispan <1>
metadata:
  name: datagrid-service <2>
  namespace: {{ USER_ID }}-cache
spec:
  replicas: 2 <3>
  expose:
    type: LoadBalancer <4>
----

<1> Tell Kubernetes/Openshift that the Custom resource type is `Infinispan`.
<2> Specify the name of our cluster as `datagrid-service`.
<3> Specify the replicas we want for our service.
<4> Expose the grid to the outside world.

We are going to open that YAML definition again to check if monitoring is enabled.

<1> Open the Administrator perspective in the Openshift console.
<2> Navigate to `Installed Operators > Data Grid > All instances`.
<3> Click on `datagrid-service`.
<4> Finally click on the YAML tab.

By default, the monitoring should be enabled. Look for the following construct which is set to `true`:

[source, yaml]
----
metadata:
  annotations:
    infinispan.org/monitoring: 'true'
----

The following picture also shows an example.

image::monitoring-dg-yaml-view.png[Monitoring - Topology, 700]


This means that our Data Grid is exposing metrics at the /metrics endpoint for Prometheus to gather.

With Openshift 4.6+ you can now also get the Prometheus metrics. Let's take a look at the metrics from our Data Grid services. 

<1> Go to the Openshift Developer perspective in the Openshift Console.
<2> Make sure you have chosen the right namespace in this case `{{ USER_ID }}-cache`.
<3> Click Observe on the left menu, then metrics.
<4> Open the Metrics tab and confirm that you can query Data Grid metrics, via the custom query with the following text and press enter.

[source, shell, role="copypaste"]
----
vendor_cache_manager_default_cluster_size
----

The following picture shows an example view showing us that we have 2 replicas in our cluster.

image::monitoring-dg-metrics-view.png[Monitoring - Topology, 700]

You can try a bunch of other queries using the custom query area. 

Perfect so we have Prometheus metrics working in a well-integrated environment. Let's also configure a Grafana dashboard.

To support various needs, the Data Grid Operator integrates with the community version of the Grafana Operator to create dashboards for Data Grid services.
Grafana is an open-source solution that enables pulling up metrics. It makes the data more meaningful by organizing it via multiple Datasources from a distributed deployment. E.g if we have a bunch of Data grid clusters we could use this dashboard to track and monitor all of them. Grafana dashboards make this possible. 
 
Let's configure our Grafana dashboard. The Data grid operator will take care of this for us. But first, we will need to create a Grafana instance so the operator knows where to configure the Data Grid dashboard.

First, let's create a Service Account for our application, press the `+` sign on the top right corner and load the following YAML. Press create. Also shown in the following picture.

image::monitoring-create-sa-yaml.png[Monitoring - Topology, 500]

[source, yaml, role="copypaste"]
----
apiVersion: v1
kind: ServiceAccount
metadata:
  name: infinispan-monitoring
  namespace: {{ USER_ID }}-cache
----


Now, we have set up a service account that will enable Grafana to read the Data Grid metrics.

Let's set up a Grafana instance as well. 

Press the `+` sign on the top right corner in the Openshift Console and load the following CR YAML to create a Grafana instance. 

[source, yaml, role="copypaste"]
----
apiVersion: integreatly.org/v1alpha1
kind: Grafana
metadata:
  name: grafana
spec:
  config:
    auth:
      disable_signout_menu: true
    auth.anonymous:
      enabled: true
    log:
      level: warn
      mode: console
    security:
      admin_password: secret
      admin_user: root
  ingress:
    enabled: true
  dashboardLabelSelector:
    - matchExpressions:
        - key: app
          operator: In
          values:
            - grafana
----

In order to ensure that we can get metrics from Data Grid into Grafana via Prometheus we will set up a Datasource. In the Openshift console, ensure you are in the project: {{ USER_ID }}-cache and then press the `+` sign on the top right corner and copy the following YAML. 

[source, shell, role="copypaste"]
----
apiVersion: integreatly.org/v1alpha1
kind: GrafanaDataSource
metadata:
  name: grafanadatasource
spec:
  name: datasource.yaml
  datasources:
    - access: proxy
      editable: true
      isDefault: true
      jsonData:
        httpHeaderName1: Authorization
        timeInterval: 5s
        tlsSkipVerify: true
      name: Prometheus
      secureJsonData:
        httpHeaderValue1: >-
          Bearer
          <YOUR  BEARER TOKEN HERE>
      type: prometheus
      url: 'https://thanos-querier.openshift-monitoring.svc.cluster.local:9091'
----

WARNING: You need to replace the value of the bearer token.

For this, head off to your CodeReady workspaces Terminal and run the following command in the CodeReady workspaces terminal. Ensure that you are already logged in to Openshift via the terminal and that your project is `{{ USER_ID }}-cache`.

[source, shell, role="copypaste"]
----
oc serviceaccounts get-token infinispan-monitoring
----

The output should give you a large String which is the actual token that we will use for the Grafana Datasource. Copy this token and head back to the Openshift console and replace the `<YOUR  BEARER TOKEN HERE>` with your actual token. The token is long and encrypted. It enables Grafana to integrate with Data Grid. Press create.

Perfect! You are moving along nicely. One final step. Let's configure our dashboard. Data Grid should be able to watch this namespace e.g. in case Grafana was in another namespace. In our case, it isn't. So all we need to do is create a dashboard YAML. Again click the `+` sign on the top right corner of your Openshift console to create a new YAML config.


[source, shell, role="copypaste"]
----
apiVersion: v1
kind: ConfigMap
metadata:
  name: infinispan-operator-config
data:
  grafana.dashboard.namespace: {{ USER_ID }}-cache
  grafana.dashboard.name: infinispan
  grafana.dashboard.monitoring.key: middleware
----

Press `create` and this should create a new dashboard in Grafana that will be called `Infinispan`. All Data Grid instances in our namespace will be sending data to Prometheus which is then loaded up in the Grafana dashboard. If this CR is removed, the dashboard will stop existing.

Head over to `Networking > Routes`, and click the Grafana route. You should see the Grafana landing page. As shown in the picture below click the Manage menu and you should be able to see `{{ USER_ID }}-cache` and then under it the link to the `Infinispan` dashboard. 

image::monitoring-grafana-dashboard-1.png[Monitoring - Topology, 700]

Click on `Infinispan` and the following dashboard should load up. the metrics details might differ.

image::monitoring-grafana-dashboard-2.png[Monitoring - Topology, 700]


=== Recap
<1> Service monitor via the DataGrid Operator
<2> Metrics via Openshift console and prometheus
<4> Created Grafana Datasource
<5> Created Grafana Dashboard


*Congratulations!!* you have completed the metrics and monitoring labs. All the instances that will be created in this namespace etc will show up in the metrics and the dashboard. Let's move on to the next labs and do exactly that! 