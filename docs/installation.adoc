=== Deployment: What's an Operator and how does it help us?
:experimental:

An Operator is a method of packaging, deploying, and managing a Kubernetes-native application. A Kubernetes-native application is an application that is both deployed on Kubernetes and managed using the Kubernetes APIs and tooling. An Operator is essentially a custom controller and encapsulates operational knowledge.

A controller is a core concept in Kubernetes and is implemented as a software loop that runs continuously on Kubernetes comparing, and if necessary, reconciling the expressed desired state and the current state of an object. Objects are well known resources like `Pods`, `Services`, `ConfigMaps`, or `PersistentVolumes`. Operators apply this model at the level of entire applications and are, in effect, application-specific controllers.

The Operator is a piece of software running in a `Pod` on the cluster, interacting with the Kubernetes API server. It introduces new object types through Custom Resource Definitions, an extension mechanism in Kubernetes. These custom objects are the primary interface for a user; consistent with the resource-based interaction model on the Kubernetes cluster.

An Operator watches for these custom resource types and is notified about their presence or modification. When the Operator receives this notification it will start running a loop to ensure that all the required connections for the application service represented by these objects are actually available and configured in the way the user expressed in the objectâ€™s specification.

The Operator Lifecycle Manager (OLM) is the backplane that facilitates management of operators on a Kubernetes cluster. Operators that provide popular applications as a service are going to be long-lived workloads with, potentially, lots of permissions on the cluster.

With OLM, administrators can control which Operators are available in what namespaces and who can interact with running Operators. The permissions of an Operator are accurately configured automatically to follow a least-privilege approach. OLM manages the overall lifecycle of Operators and their resources, by doing things like resolving dependencies on other Operators, triggering updates to both an Operator and the application it manages, or granting a team access to an Operator for their slice of the cluster.

Red Hat Data Grid 8.2 comes with an Operator. The administrators of the cluster you are working with today have already installed the Data Grid Operator. What we need to do as a user is define a Custom Resource as to how and what configuration we want for our Red Hat Data Grid instances. 

=== Installing
If you have not already logged into OpenShift from the CodeReady Workspaces terminal, please do that now.Click on the `Login to Openshift` menu in the right menu called 'My Workspace'. When prompted, your *Username* is `{{ USER_ID }}` and your *Password* is `{{ OPENSHIFT_USER_PASSWORD }}`.

Open a new browser to the link:{{ CONSOLE_URL }}[OpenShift web console^]

image::openshift_login.png[openshift_login, 700]

Login using:

* Username: `{{ USER_ID }}`
* Password: `{{ OPENSHIFT_USER_PASSWORD }}`

Once Logged in, goto your project `{{ USER_ID }}-cache`
Click the link Installed Operator on the left, as shown in the picture below.

image::dg_operatorinstalled.png[openshift_login, 700]

Notice that the DataGrid operator is already installed in your namespace. 

image::dg_operatoroverview.png[openshift_login, 700]

You can see there are no clusters installed in our namespace. Let's go ahead and do that. 

Click on `Create datagrid` and replace the following YAML with the default sample: 

[source, yaml, role="copypaste"]
----
apiVersion: infinispan.org/v1
kind: Infinispan <1>
metadata:
  name: datagrid-service <2>
  namespace: {{ USER_ID }}-cache
spec:
  replicas: 2 <3>
----

<1> Tell Kubernetes/Openshift that the Custom resource type is datagrid
<2> Specify the name of our cluster as datagrid-service
<3> Specify the replicas we want for our service

Also notice that we are calling our service `datagrid-service`, we will use this name in the following labs to access our cluster.

Click *Create* at the bottom.

Check that datagrid nodes have successfully formed clusters.

Now let's retrieve the DataGrif CR for DataGrid Operator.

[source, shell, role="copypaste"]
----
kubectl get infinispan -o yaml
----

The response indicates that datagrid nodes have received clustered views, as in the following example:

[source, shell]
----
conditions:
  - message: 'View: [datagrid-service-0, datagrid-service-1]'
    status: "True"
    type: wellFormed
----


You can also wait for the condition check:

[source, shell, role="copypaste"]
----
kubectl wait --for condition=wellFormed --timeout=240s infinispan/datagrid-service
----

Let's retrieve cluster view from logs as follows:
[source, shell, role="copypaste"]
----
kubectl logs datagrid-service-0 | grep ISPN000094
----

[source, shell]
----
INFO  [org.infinispan.CLUSTER] (MSC service thread 1-2) \
ISPN000094: Received new cluster view for channel datagrid-service: \
[datagrid-service-0|0] (1) [datagrid-service-0]

INFO  [org.infinispan.CLUSTER] (jgroups-3,datagrid-service-0) \
ISPN000094: Received new cluster view for channel datagrid-service: \
[datagrid-service-0|1] (2) [datagrid-service-0, datagrid-service-1]
----

You can also look for the pods running the Red Hat Data Grid Operator and the instances by running the following command:

[source, shell, role="copypaste"]
----
oc get pods
----

Above command should render a similar output as below:

[source, shell]
----
[jboss@workspacel7b3gw19zpoclvcu dg8-operator]$ oc get pods
NAME                                   READY   STATUS    RESTARTS   AGE
datagrid-service-0                     1/1     Running   0          2m59s
datagrid-service-1                     1/1     Running   0          2m14s
Data Grid-operator-544ff55c59-4s7wl   1/1     Running   1          2d10h
----

All looks great! how about we also check the ClusterIP and its ports etc.
[source, shell, role="copypaste"]
----
oc get services
----

The above command should render a similar output as shown in the example below. Showing all the services:

[source, shell]
----
NAME                        TYPE           CLUSTER-IP       EXTERNAL-IP                                                                    PORT(S)           AGE
datagrid-service            ClusterIP      172.30.115.185   <none>                                                                         11222/TCP         5m55s
datagrid-service-external   LoadBalancer   172.30.90.75     abd9b45a50a174648af684c05cba0bd9-1926931502.ap-southeast-1.elb.amazonaws.com   11222:32206/TCP   5m55s
datagrid-service-ping       ClusterIP      None             <none>                                                                         8888/TCP          5m55s
----

You can see that there are three datagrid-services, 

- 1 for use within the cluster, 
- 1 for ping service which ensures that the clusters are healthy and operational 
- and lastly the external service, which we will use to goto the Admin console.


An Operator updates the installation on the fly, it ensures it can keep the correct state of the cluster at all times. So one should not need to change specific cluster config but define them via the custom resource (CR) which the operator is always watching. Lets try this out. How about adding an external route to our `datagrid-service`.

Lets edit the datagrid-service CR. 

As shown in the picture below, click on `Edit datagrid`

image::dg_edit_CR.png[Edit CR, 700]

The above should load the yaml with some additional information e.g. timestamp, labels etc that were added by the operator once the cluster instance was created. 


We will make changes to the cluster `Spec:`, navigate your cursor to `Replicas` under spec and add the following as shown in the picture below.

[source, shell, role="copypaste"]
----
  expose:
    type: LoadBalancer
----

image::dg_edit_CR_LoadBalancer.png[Edit and Save, 700]

Perfect now press `save`

Now if you navigate back to `Installed Operators > Operator Details` and then click `datagrid-service` you should see the following page with the expose LoadBalancer link to your Data Grid Console.

image::dg_CR_detailview.png[DG cluster detail view, 700]


The following is an example, your `LoadBalancer` url will most likely differ:

* `http://ad6cd35d6e6aa46fcb96558204c35f08-872149037.us-east-1.elb.amazonaws.com:11222`


If you try to access the url; you would need to provide credentials. 

The datagrid operator creates the credentials during installation time and they should be stored in your namespace secrets. Head back your CodeReady Workspace terminal. Let's get the secret with the following command.

[source, shell, role="copypaste"]
----
oc get secret datagrid-service-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode
----

And now the final test to check we have a running cluster; login with the username developer and the password from the above secret.

image::dg_adminconsole.png[openshift_login, 900]


==== Stopping and starting datagrid clusters
How to stop and start Datagrid nodes in a graceful, ordered fashion to correctly preserve cluster state.

Clusters of Data Grid Service nodes must restart with the same number of nodes that existed before shutdown. This allows datagrid to restore the distribution of data across the cluster. After datagrid Operator fully restarts the cluster you can safely add and remove nodes.

Let's change the CR by changing spec.replicas field to 0 to stop the datagrid cluster.

[source, shell, role="copypaste"]
----
spec:
  replicas: 0
----

Ensure you have the correct number of nodes before you restart the cluster.

[source, shell]
----
$ kubectl get Data Grid example-Data Grid -o=jsonpath='{.status.replicasWantedAtRestart}'
----

Change the spec.replicas field to the same number of nodes to restart the datagrid cluster.

[source, shell, role="copypaste"]
----
spec:
  replicas: 2
----

==== Types of Data Grid services

Data Grid has two types of services 

<1> Cache Service
<2> DataGrid Service

Services are stateful applications, based on the Data Grid Server image, that provide flexible and robust in-memory data storage. If you do not e.g. specify a value for the spec.service.type field, Data Grid Operator creates Cache Service nodes by default. Each service has different benefits and enables applications to leverage the different features exposed by the Data grid.

*Cache Service*

* Use Cache Service if you want a volatile, low-latency data store with minimal configuration. Cache Service nodes:
* Automatically scale to meet capacity when data storage demands go up or down.
* Synchronously distribute data to ensure consistency.
* Replicates each entry in the cache across the cluster.
* Store cache entries off-heap and use eviction for JVM efficiency.
* Ensure data consistency with a default partition handling configuration.
* Because Cache Service nodes are volatile you lose all data when you apply changes to the cluster with the Data Grid CR or update the Data Grid version.

*Data Grid Service*

* Back up data across global clusters with cross-site replication.
* Create caches with any valid configuration.
* Add file-based cache stores to save data in a persistent volume.
* Query values across caches using the Data Grid Query API.
* Use advanced Data Grid features and capabilities.

You might have noticed that in our current example in this section we used the Cache service, in the upcoming labs we will configure the different services and features entailed thereof.


=== Recap
<1> You created your first CR
<2> Deployed the CR to Openshift using the DataGrid operator
<3> You installed your first DataGrid instance
<4> Exposed the service to the outside world
<5> Learnt how to stop and start the DataGrid via CR, and track the status/logs
<6> Differences between the two types of services

*Congratulations!!* you have completed the first Datagrid installation of this workshop. Let's move to the next lab and learn how we can use this instance as a RemoteCache with a Quarkus Application.

