=== Deployment: Geo Distributed Cache

Data Grid Operator in one data center can discover a Data Grid cluster that Data Grid Operator manages in another data center. This discovery allows Data Grid to automatically form cross-site views and create global clusters.

The following illustration provides an example in which Data Grid Operator manages a Data Grid cluster at a data center in New York City, NYC. At another data center in London, LON, Data Grid Operator also manages a Data Grid cluster.

image::xsite-rhdg.png[Cross site - Topology, 700]


Data Grid Operator uses the Kubernetes API to establish a secure connection between the OpenShift Container Platform clusters in NYC and LON. Data Grid Operator then creates a cross-site replication service so Data Grid clusters can back up data across locations.

Each Data Grid cluster has one site master node that coordinates all backup requests. Data Grid Operator identifies the site master node so that all traffic through the cross-site replication service goes to the site master.

If the current site master node goes offline then a new node becomes site master. Data Grid Operator automatically finds the new site master node and updates the cross-site replication service to forward backup requests to it.

In our case since this is a lab, we will not make use of two different clusters, but to understand how this works, we will use two different namespaces. 
The concepts will be very much similar. Although this might make for a a great showcase, its a very unique usecase to use Cross-site replication in two namespaces. 
We will use two projects/namespaces

<1> {{ USER_ID }}-cache - Site (NYC)
<2> {{ USER_ID }}-cache2 - Site (LON)


Open the `OpenShift` console with the link:{{ CONSOLE_URL }}[OpenShift web console^]

Choose the project {{ USER_ID }}-cache

And create new Data Grid instance by Navigating to `Installed Operators > Data Grid > Create Infinispan ` as the following pictures shows

image::create-infinispan.png[Cross site - Topology, 700]

Press the buttone `Create Infinispan`

First we will create a new Infinispan cluster for our NYC location in the {{ USER_ID }}-cache

image::xsite-nyc-yaml.png[Cross site - Topology, 700]

Load the below yaml and press `Create` as shown in the picture above ^

[source, yaml, role="copypaste"]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: example-nyc
spec:
  replicas: 2
  logging:
    categories:
      org.jgroups.protocols.TCP: error
      org.jgroups.protocols.relay.RELAY2: error
  service:
    container:
      storage: 1Gi
    sites:
      local:
        expose:
          type: ClusterIP
        name: NYC
      locations:
        - clusterName: example-lon
          name: LON
          namespace: {{ USER_ID }}-cache2
          secretName: lon-token
          url: 'infinispan+xsite://example-lon-site.{{ USER_ID }}-cache2.svc:7900'    
    type: DataGrid
----


And lets the the same for our LON site in project `{{ USER_ID }}-cache2`

image::xsite-lon-yaml.png[Cross site - Topology, 700]

Load the below yaml and press `Create` as shown in the picture above ^


[source, yaml, role="copypaste"]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: example-lon
spec:
  replicas: 2
  logging:
    categories:
      org.jgroups.protocols.TCP: error
      org.jgroups.protocols.relay.RELAY2: error
  service:
    container:
      storage: 1Gi
    sites:
      local:
        expose:
          type: ClusterIP
        name: LON
      locations:
        - clusterName: example-nyc
          name: NYC
          namespace: {{ USER_ID }}-cache
          secretName: lon-token
          url: 'infinispan+xsite://example-nyc-site.{{ USER_ID }}-cache.svc:7900'    
    type: DataGrid    
----

Once the cluster starts to provision via the Operator you should see it listed as follows

image::xsite-nyc-screenshot.png[Cross site - Topology, 700]

Click on the cluster name and then click on `YAML`
This should give you the details of the CR but also most importantly the status

image::xsite-nyc-status-yaml.png[Cross site - Topology, 700]


Scroll all the way down to the bottom of the yaml and check the status tag; should look similar to the following `YAML`

[source, yaml, role="copypaste"]
----
status:
  conditions:
    - status: 'True'
      type: PreliminaryChecksPassed
    - message: 'View: example-nyc-0-63972,example-nyc-1-45654'
      status: 'True'
      type: WellFormed
    - message: 'Cross-Site view: LON'
      status: 'True'
      type: CrossSiteViewFormed
  podStatus:
    ready:
      - example-nyc-1
      - example-nyc-2
  statefulSetName: example-nyc
----

Now that we have a cross site configured how about creating the a replicated cache with a backup and loading some data to see how this is working. 
We will take London(LON) as the primary site and New york city (NYC) as the secondary site in this example.


This exercise builds up from the previous exercises. Its time to challenge yourself!

- Add LoadBalancer to both the site clusters. 
- Get the LoadBalancer address for the both sites 
- Get the password for the user developer for both sites

If you havent been able to figure it out, take a look at the following commands that will print the details out to you on the terminal. 


*Solution*
[source, bash]
----
# change to the project
oc project {{ USER_ID }}-cache2

# get the console url for site LON
echo "https://$(oc get services | grep example-lon-external | awk '{ print $4 }'):11222"

# get the password for user developer to login to the console
echo "$(oc get secret example-lon-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode | grep password | awk '{ print $2 }' )"
----

Perfect now that we have the details lets open up the console and setup the cross site caches. 

image::dg8_createcache_console.png[Create cache, 700]

After pressing Create cache you will see the following form. In the field `Provide Cache configuration` add the following xml and press create

image::dg8_createcacheform_console.png[Create cache, 700]

[source, xml]
----
<infinispan>
  <cache-container>
    <distributed-cache name="xsiteCache"> <1> 
      <encoding media-type="application/x-protostream"/> <2> 
      <backups>
        <backup site="NYC" strategy="SYNC"> <3> 
          <take-offline min-wait="120000"/> <4>
        </backup>
      </backups>
    </distributed-cache>
  </cache-container>
</infinispan>
----

<1> Name of our cache
<2> The media type what will be in the cache, you can choose xml, json, java serialization etc as well
<3> Here we define NYC as our backup and saying that it should be synchronous
<4> Incase the cluster is not working wait for the specified time and take it offline.

Lets do the same for our site NYC as well.

[source, bash]
----
# change to the project
oc project {{ USER_ID }}-cache

# get the console url for site NYC
echo "https://$(oc get services | grep example-nyc-external | awk '{ print $4 }'):11222"

# get the password for user developer to login to the console
echo "$(oc get secret example-nyc-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode | grep password | awk '{ print $2 }' )"
----


Repeat the step for creating the cache again but with the following xml this time, which is taking LON as the backup site.
[source, xml]
----
<infinispan>
  <cache-container>
    <replicated-cache name="xsiteCache">
      <encoding media-type="application/x-protostream"/>
      <backups>
        <backup site="LON" strategy="ASYNC" >
          <take-offline min-wait="120000"/>
        </backup>
      </backups>
    </replicated-cache>
  </cache-container>
</infinispan>

----

Lets create a couple of entries 
To add an entry press the `xsiteCache` on the main console page and then press `Add Entry` as shown in the image below.
Key `1` , value `Coffee`

image::dg8_createentry_console.png[Create cache, 700]

Now if you go to the NYC console and cache details you should be able to see the same entry as shown in the image below. 

image::dg8_createentryoutput_console.png[Create cache, 700]



*Challenge yourself*

- Add an entry that will live for 5 seconds
- Add an entry that will never die
- Add an entry that will have a max idle time for 5 seconds. This means the entry will die if there is read and write operations on it.
- Add an entry with key type as integer and value as String

Perfect now that the caches are created. Browse to the Console and there you should be able to see the cache name `xsiteCache` on both the clusters. 


If you will visit the console on any of the clusters you should be able to view the entries. 

=== Recap
<1> Created two sites in two different namespaces
<2> Deployed the CR to Openshift using the DataGrid operator
<4> Exposed the service to the outside world
<5> Created a replicated cache over the two sites
<6> Loaded data into the Cache and saw how its replicated over
<7> Learnt the different parameters used from the console to setup an entry

*Congratulations!!* you have completed the first Cross site Datagrid installation of this workshop. Let's move to the next lab!