=== Deployment: Geo-Distributed Cache

Data Grid Operator in one data center can discover a Data Grid cluster that another Data Grid Operator manages in another data center. This discovery allows Data Grid to automatically form cross-site views and create global clusters.

The following illustration provides an example in which Data Grid Operator manages a Data Grid cluster at a data center in New York City, NYC. At another data center in London, LON, a Data Grid Operator also manages a Data Grid cluster.

image::xsite-rhdg.png[Cross site - Topology, 700]


Data Grid Operator uses the Kubernetes API to establish a secure connection between the OpenShift Container Platform clusters in NYC and LON. Data Grid Operator then creates a cross-site replication service so that Data Grid clusters can back up data across locations.

Each Data Grid cluster has one site master node that coordinates all backup requests. Data Grid Operator identifies the site master node so that all traffic through the cross-site replication service goes to the site master.

If the current site master node goes offline, a new node becomes site master. Data Grid Operator automatically finds the new site master node and updates the cross-site replication service to forward backup requests to it.

Since this is a lab, we will not use two different clusters, but to understand how this works, we will use two different namespaces. 
The concepts will be very much similar. Although this might make for a great showcase, its a very unique use case to use Cross-site replication in two namespaces. 
We will use two projects/namespaces:

<1> {{ USER_ID }}-cache - Site (NYC)
<2> {{ USER_ID }}-cache2 - Site (LON)


Open the `OpenShift` console with the link:{{ CONSOLE_URL }}[OpenShift web console^]

Choose the project {{ USER_ID }}-cache

And create a new Data Grid instance by Navigating to `Installed Operators > Data Grid > Create Infinispan ` as the following picture shows:

image::create-infinispan.png[Cross site - Topology, 700]

Press the button `Create Infinispan`.

First, we will create a new Infinispan cluster for our NYC location in the {{ USER_ID }}-cache

image::xsite-nyc-yaml.png[Cross site - Topology, 700]

Copy the below YAML and press `Create` as shown in the picture above:

[source, yaml, role="copypaste"]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: example-nyc
spec:
  replicas: 2
  logging:
    categories:
      org.jgroups.protocols.TCP: error
      org.jgroups.protocols.relay.RELAY2: error
  service:
    container:
      storage: 1Gi
    sites:
      local:
        expose:
          type: ClusterIP
        name: NYC
      locations:
        - clusterName: example-lon
          name: LON
          namespace: {{ USER_ID }}-cache2
          secretName: lon-token
          url: 'infinispan+xsite://example-lon-site.{{ USER_ID }}-cache2.svc:7900'    
    type: DataGrid
----


And let's do the same for our LON site in the project `{{ USER_ID }}-cache2`:

image::xsite-lon-yaml.png[Cross site - Topology, 700]

Load the YAML shown below and press `Create` as shown in the picture above:


[source, yaml, role="copypaste"]
----
apiVersion: infinispan.org/v1
kind: Infinispan
metadata:
  name: example-lon
spec:
  replicas: 2
  logging:
    categories:
      org.jgroups.protocols.TCP: error
      org.jgroups.protocols.relay.RELAY2: error
  service:
    container:
      storage: 1Gi
    sites:
      local:
        expose:
          type: ClusterIP
        name: LON
      locations:
        - clusterName: example-nyc
          name: NYC
          namespace: {{ USER_ID }}-cache
          secretName: lon-token
          url: 'infinispan+xsite://example-nyc-site.{{ USER_ID }}-cache.svc:7900'    
    type: DataGrid    
----

Once the cluster starts to provision via the operator you should see it listed as follows:

image::xsite-nyc-screenshot.png[Cross site - Topology, 700]

Click on the cluster name and then click on `YAML`. This YAML should give you the details of the CR but, most importantly, its status.

image::xsite-nyc-status-yaml.png[Cross site - Topology, 700]


Scroll all the way down to the bottom of the yaml and check the status tag; should look similar to the following `YAML`.

[source, yaml, role="copypaste"]
----
status:
  conditions:
    - status: 'True'
      type: PreliminaryChecksPassed
    - message: 'View: example-nyc-0-63972,example-nyc-1-45654'
      status: 'True'
      type: WellFormed
    - message: 'Cross-Site view: LON'
      status: 'True'
      type: CrossSiteViewFormed
  podStatus:
    ready:
      - example-nyc-1
      - example-nyc-2
  statefulSetName: example-nyc
----

Now that we have cross site configured, how about creating a replicated cache with a backup and loading some data to see how this works. 
We will take London(LON) as the primary site and New York city (NYC) as the secondary site.


This exercise builds up from the previous exercises. It's time to challenge yourself!

- Add LoadBalancer to both the site clusters. 
- Get the LoadBalancer address for both sites.
- Get the password for the user developer for both sites.

If you haven't been able to figure it out, take a look at the following commands that will print the details out to you on the terminal.


*Solution*
[source, bash]
----
# change to the project
oc project {{ USER_ID }}-cache2

# get the console url for site LON
echo "https://$(oc get services | grep example-lon-external | awk '{ print $4 }'):11222"

# get the password for user developer to login to the console
echo "$(oc get secret example-lon-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode | grep password | awk '{ print $2 }' )"
----

Perfect! Now that we have the details let's open up the console and set up the cross-site caches. 

image::dg8_createcache_console.png[Create cache, 700]

After pressing `Create cache` you will see the following form. In the field `Provide Cache configuration` add the following XML and press create:

image::dg8_createcacheform_console.png[Create cache, 700]

[source, xml]
----
<infinispan>
  <cache-container>
    <distributed-cache name="xsiteCache"> <1> 
      <encoding media-type="application/x-protostream"/> <2> 
      <backups>
        <backup site="NYC" strategy="SYNC"> <3> 
          <take-offline min-wait="120000"/> <4>
        </backup>
      </backups>
    </distributed-cache>
  </cache-container>
</infinispan>
----

<1> Name of our cache.
<2> The media type of the cache. You can choose XML, JSON, Java Serialization, etc.
<3> Here, we define NYC as our backup and say it should be synchronous.
<4> In case the cluster is not working, wait for the specified time and take it offline.

Let's do the same for our site in NYC as well.

[source, bash]
----
# change to the project
oc project {{ USER_ID }}-cache

# get the console url for site NYC
echo "https://$(oc get services | grep example-nyc-external | awk '{ print $4 }'):11222"

# get the password for user developer to login to the console
echo "$(oc get secret example-nyc-generated-secret -o jsonpath="{.data.identities\.yaml}" | base64 --decode | grep password | awk '{ print $2 }' )"
----


Repeat the step to create the cache again but with the following XML. This time, LON is the backup site.

[source, xml]
----
<infinispan>
  <cache-container>
    <replicated-cache name="xsiteCache">
      <encoding media-type="application/x-protostream"/>
      <backups>
        <backup site="LON" strategy="ASYNC" >
          <take-offline min-wait="120000"/>
        </backup>
      </backups>
    </replicated-cache>
  </cache-container>
</infinispan>

----

Let's create a couple of entries.
To add an entry, press the `xsiteCache` on the main console page and then press `Add Entry` as shown in the image below. Use Key `1` and value `Coffee`.

image::dg8_createentry_console.png[Create cache, 700]

Now, if you go to the NYC console and check the cache details site, you should see the entry shown in the image below. 

image::dg8_createentryoutput_console.png[Create cache, 700]



*Challenge yourself*

- Add an entry that will live for 5 seconds.
- Add an entry that will never die.
- Add an entry with a max idle time of 5 seconds, which means that the entry will die if there aren't read and write operations on it.
- Add an entry with key type Integer and value type String.

Perfect! Now that the caches are created, browse to the web console where you should see the cache name `xsiteCache` on both clusters. 


If you visit the console on any of the clusters you should be able to view the entries. 

=== Recap
<1> Created two sites in two different namespaces.
<2> Deployed the CR to Openshift using the DataGrid operator.
<3> Exposed the service to the outside world.
<4> Created a replicated cache over the two sites.
<5> Loaded data into the Cache and saw how its replicated over.
<6> Learnt the different parameters used from the console to setup an entry.

*Congratulations!!* you have completed your first Cross site Data Grid installation of this workshop. Let's move to the next lab!
